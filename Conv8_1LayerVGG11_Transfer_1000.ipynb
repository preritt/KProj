{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv8_1LayerVGG11_Transfer_1000.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/KProj/blob/master/Conv8_1LayerVGG11_Transfer_1000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdTAcjYec3s7",
        "colab_type": "text"
      },
      "source": [
        "## Load standard libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCo6LRTTcxmC",
        "colab_type": "code",
        "outputId": "9cac8fcd-ce40-40e5-b24f-0634cafabb86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import gzip\n",
        "%matplotlib inline\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model,Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import cifar10\n",
        "from sklearn import preprocessing\n",
        "from keras import losses\n",
        "from keras.models import model_from_json\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg_ZApCOPN9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG3txkjndHWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rDrYIgZc20A",
        "colab_type": "text"
      },
      "source": [
        "## Load image data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZUnD2K0ErdL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "126dc7fc-c71f-447a-9690-f1f0900f8ae4"
      },
      "source": [
        "!pip install scipy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "585DNuo-Em2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from scipy.misc import toimage\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85Dlxs7adKVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot\n",
        "# from scipy.misc import toimage\n",
        "from keras.datasets import cifar10\n",
        "# def show_imgs(X):\n",
        "#     pyplot.figure(1)\n",
        "#     k = 0\n",
        "#     for i in range(0,4):\n",
        "#         for j in range(0,4):\n",
        "#             pyplot.subplot2grid((4,4),(i,j))\n",
        "#             pyplot.imshow(toimage(X[k]))\n",
        "#             k = k+1\n",
        "#     # show the plot\n",
        "#     pyplot.show()\n",
        " \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# show_imgs(x_test[:16])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQQPlF6RdS2m",
        "colab_type": "text"
      },
      "source": [
        "## Create Function to get data corresponding to three labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFyk3xzNdYNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function takes the entire traaining data and test data and corresponding categorical label and returns the corresponding data in class indices\n",
        "def getThreeClassesTrainTest(class_indices,train_data,train_labels,test_data,test_labels):\n",
        "  all_train_data = []\n",
        "  all_train_data_label = []\n",
        "  all_test_data = []\n",
        "  all_test_data_label = []\n",
        "  for ind in class_indices:\n",
        "    this_class_location_train =  np.where(train_labels==ind)[0] # train_labels[train_labels == ind]\n",
        "    this_class_data_train = train_data[this_class_location_train,:]\n",
        "    this_class_data_train_label = train_labels[this_class_location_train]\n",
        "#     print(this_class_location_train)\n",
        "    \n",
        "    this_class_location_test = np.where(test_labels==ind)[0] #test_labels[test_labels == ind]\n",
        "    this_class_data_test = test_data[this_class_location_test,:]\n",
        "    this_class_data_test_label = test_labels[this_class_location_test]\n",
        "    \n",
        "    all_train_data.append(this_class_data_train)\n",
        "    all_test_data.append(this_class_data_test)\n",
        "    all_train_data_label.append(this_class_data_train_label)\n",
        "    all_test_data_label.append(this_class_data_test_label)\n",
        "    \n",
        "  train_data_three = all_train_data[0]\n",
        "  train_label_three = all_train_data_label[0]\n",
        "  test_data_three = all_test_data[0]\n",
        "  test_label_three = all_test_data_label[0]\n",
        "  \n",
        "  for j in range(1,len(class_indices)):\n",
        "    train_data_three = np.concatenate((train_data_three,all_train_data[j]))\n",
        "    train_label_three = np.concatenate((train_label_three,all_train_data_label[j]))\n",
        "\n",
        "    test_data_three = np.concatenate((test_data_three,all_test_data[j]))\n",
        "    test_label_three = np.concatenate((test_label_three,all_test_data_label[j]))\n",
        "#   print(train_label_three.shape)\n",
        "  train_label_three = np.reshape(train_label_three,newshape=(-1,))\n",
        "  test_label_three = np.reshape(test_label_three,newshape=(-1,))\n",
        "  return train_data_three, train_label_three, test_data_three,test_label_three"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex0j41PLde2x",
        "colab_type": "text"
      },
      "source": [
        "## Get data for three classes using getThreeClassesTrainTest function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpqbOTfBdhHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_three_data,train_three_label,test_three_data,test_three_label = getThreeClassesTrainTest([ 0,3,5 ],x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddFucSl_YW5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_three_data,train_three_label,test_three_data,test_three_label = (np.load('train_three_data_saved.npy'),\n",
        "                                                                      np.load('train_three_label_saved.npy'),\n",
        "                                                                      np.load('test_three_data_saved.npy'),\n",
        "                                                                      np.load('test_three_label_saved.npy'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDQiwalsYIGo",
        "colab_type": "code",
        "outputId": "57192445-73a1-43ff-96f5-d54d9ec17f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_three_label"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 5, ..., 0, 5, 5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCQfV4TvZh1r",
        "colab_type": "code",
        "outputId": "c96ab325-f9dc-45fd-e7e7-6a54af9a167a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_three_label"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 5, 0, ..., 5, 0, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gK_tiIWd2Nf",
        "colab_type": "code",
        "outputId": "54ee45af-d855-41ad-bf70-b619331ea025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "plt.hist(train_three_label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([5000.,    0.,    0.,    0.,    0.,    0., 5000.,    0.,    0.,\n",
              "        5000.]),\n",
              " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/gESZomJ0QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEYcIMeqd4i1",
        "colab_type": "code",
        "outputId": "2dcdda01-ee8d-4b9b-89e5-06d094afcc9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "plt.hist(test_three_label)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1000.,    0.,    0.,    0.,    0.,    0., 1000.,    0.,    0.,\n",
              "        1000.]),\n",
              " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKlTBkCSOmUAJKlTBkCSOvW/lK9vO4tP9cMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4n_WiMNd_Hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_three_data, train_three_label = shuffle(train_three_data, train_three_label )\n",
        "# test_three_data,test_three_label = shuffle(test_three_data,test_three_label )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT_AKqQmeBk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "le.fit_transform(train_three_label)\n",
        "train_three_label_cat = le.transform(train_three_label)\n",
        "test_three_label_cat = le.transform(test_three_label)\n",
        "train_three_label_one_hot = to_categorical(train_three_label_cat)\n",
        "test_three_label_one_hot = to_categorical(test_three_label_cat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWEjK_exeEaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split train data and to create validation data\n",
        "train_X,valid_X,train_ground,valid_ground = train_test_split(train_three_data,\n",
        "                                                             train_three_label_one_hot,\n",
        "                                                             test_size=0.2,\n",
        "                                                             random_state=13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOFKlZWKeH25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = test_three_label_one_hot.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr931MtCYGoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssi32yj-d1iX",
        "colab_type": "text"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUISHV2weN0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = 32, 32\n",
        "inChannel = 3\n",
        "input_img = Input(shape = (x, y, inChannel))\n",
        "ep = 10 \n",
        "weight_decay = 1e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ich0RbYnebwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cifar10Classifier(input_img):\n",
        "  \n",
        "  conv1_1 = (Conv2D(64, (3,3),name = 'conv1_1' ,padding='same', \n",
        "                    kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:],\n",
        "                    activation='relu'))(input_img)\n",
        "  conv1_2 = BatchNormalization(name = 'conv1_2')(conv1_1)\n",
        "  pool1_3 = MaxPooling2D(pool_size=(2, 2), name = 'pool1_3')(conv1_2) #14 x 14 x 32\n",
        "  conv2_1 = (Conv2D(128, (3,3),name = 'conv2_1' ,padding='same',\n",
        "                    activation= 'relu',kernel_regularizer=regularizers.l2(weight_decay)))(pool1_3)\n",
        "  conv2_2 = BatchNormalization(name = 'conv2_2')(conv2_1)\n",
        "  pool2_3 = MaxPooling2D(pool_size=(2, 2), name = 'pool2_3')(conv2_2) #14 x 14 x 32\n",
        "  \n",
        "  conv3_1 = (Conv2D(256, (3,3), padding='same',name = 'conv3_1',\n",
        "                    kernel_regularizer=regularizers.l2(weight_decay), activation='relu'))(pool2_3)\n",
        "  conv3_2 = BatchNormalization(name = 'conv3_2')(conv3_1)\n",
        "  conv4_1 = (Conv2D(256, (3,3), padding='same',name = 'conv4_1' ,activation= 'relu',\n",
        "                    kernel_regularizer=regularizers.l2(weight_decay)))(conv3_2)\n",
        "  conv4_2 = BatchNormalization(name = 'conv4_2')(conv4_1)\n",
        "  pool4_2 = MaxPooling2D(pool_size=(2, 2), name = 'pool4_2')(conv4_2) #14 x 14 x 32\n",
        "  \n",
        "  conv5_1 = (Conv2D(512, (3,3), padding='same',name = 'conv5_1',\n",
        "                    kernel_regularizer=regularizers.l2(weight_decay), activation='relu'))(pool4_2)\n",
        "  conv5_2 = BatchNormalization(name = 'conv5_2')(conv5_1)\n",
        "  conv6_1 = (Conv2D(512, (3,3), padding='same',name = 'conv6_1' ,activation= 'relu',\n",
        "                    kernel_regularizer=regularizers.l2(weight_decay)))(conv5_2)\n",
        "  conv6_2 = BatchNormalization(name = 'conv6_2')(conv6_1)\n",
        "  pool6_2 = MaxPooling2D(pool_size=(2, 2), name = 'pool6_2')(conv6_2) #14 x 14 x 32\n",
        "  \n",
        "  conv7_1 = (Conv2D(512, (3,3), padding='same',name = 'conv7_1',\n",
        "                    kernel_regularizer=regularizers.l2(weight_decay), activation='relu'))(pool6_2)\n",
        "  conv7_2 = BatchNormalization(name = 'conv7_2')(conv7_1)\n",
        "  conv8_1 = (Conv2D(512, (3,3), padding='same',name = 'conv8_1' ,activation= 'relu',\n",
        "                    kernel_regularizer=regularizers.l2(weight_decay)))(conv7_2)\n",
        "  conv8_2 = BatchNormalization(name = 'conv8_2')(conv8_1)\n",
        "  pool8_2 = MaxPooling2D(pool_size=(2, 2), name = 'pool8_2')(conv8_2) #14 x 14 x 32\n",
        "\n",
        "\n",
        "  flat = Flatten()(pool8_2)\n",
        "  fc1 = Dense(4096, activation='relu', name = 'fc1')(flat)\n",
        "  fc2 = Dense(4096, activation='relu', name = 'fc2')(fc1)\n",
        "  fc3 = Dense(1000, activation='relu', name = 'fc3')(fc2)\n",
        "\n",
        "\n",
        "  \n",
        "  out = Dense(num_classes, activation='softmax', name = 'out')(fc3)\n",
        "  \n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_0LHS-Xf7aN",
        "colab_type": "text"
      },
      "source": [
        "## Define functions for calculation of DI score\n",
        "getDIScoreOfLayer(layer_output, true_labels), \n",
        "getDIScoreOfLayerCNNLayer(cnn_outputs, true_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsLk-f7sf6zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDIScoreOfLayer(layer_output, true_labels):\n",
        "  # computeScatterMatrix\n",
        "  rho = 1e-6\n",
        "  mu = layer_output.mean(axis=0)\n",
        "#   print(mu.shape)\n",
        "  mu = mu.reshape((1,-1))\n",
        "  centered_data = layer_output - mu\n",
        "  S_bar = np.matmul(centered_data.T,centered_data)\n",
        "  # compute between class matrix\n",
        "  total_classes = true_labels.max()\n",
        "  S_B = np.zeros(shape = (mu.shape[1],mu.shape[1]))\n",
        "  S_W = np.zeros(shape = (mu.shape[1],mu.shape[1]))\n",
        "  unique_classes = np.unique(true_labels)\n",
        "  unique_classes_list = np.ndarray.tolist(unique_classes)\n",
        "  for l in unique_classes_list:\n",
        "    class_l_index = np.where(true_labels == l)\n",
        "    class_l_output = layer_output[class_l_index[0],:]\n",
        "    mu_l = class_l_output.mean(axis=0)\n",
        "    mu_l = mu_l.reshape((1,-1))\n",
        "    total_examples_this_class = class_l_index[0].shape   \n",
        "    ss = mu_l - mu\n",
        "    S_B = S_B + total_examples_this_class*np.matmul((mu_l - mu).T,(mu_l - mu))\n",
        "\n",
        "  DI_Score = np.trace(np.matmul(np.linalg.inv(S_bar + rho*np.eye(S_bar.shape[0])),S_B))\n",
        "  return DI_Score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubrpaIpFgBzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDIScoreOfLayerCNNLayer(cnn_outputs, true_labels):\n",
        "  # computeScatterMatrix\n",
        "  number_of_filters = cnn_outputs.shape[3]\n",
        "  cnn_di_scores = []\n",
        "  for filt in range(0,number_of_filters):\n",
        "    outputs_filter = cnn_outputs[:,:,:,filt]\n",
        "    outputs_filter_reshaped = np.reshape(outputs_filter, newshape=(cnn_outputs.shape[0],-1))\n",
        "    this_filter_score = getDIScoreOfLayer(outputs_filter_reshaped, true_labels)\n",
        "    cnn_di_scores.append(this_filter_score)\n",
        "  return cnn_di_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ5H0p_KjVCw",
        "colab_type": "text"
      },
      "source": [
        "## define a function to get and plot di score of a layer. It takes inputs - trained model, layer name to get DI scores and returns a plot with some statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9MHI1LFjUZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createCNNLayerDIScorePlot(trained_model, layer_name, data_to_use = train_three_data , data_label =train_three_label ):\n",
        "#   print(trained_model.layers)\n",
        "#   get_cnn1_output_function = K.function([trained_model.layers[0].input],\n",
        "#                                   [trained_model.get_layer(layer_name).output])   #get_layer test_three_label\n",
        "  \n",
        "  model = trained_model\n",
        "  intermediate_layer_model = Model(inputs=model.input,\n",
        "                                 outputs=model.get_layer(layer_name).output)\n",
        "  cnn1_output = intermediate_layer_model.predict(data_to_use)\n",
        "#   cnn1_output = get_cnn1_output_function([data_to_use])[0]\n",
        "  di_scores = getDIScoreOfLayerCNNLayer(cnn1_output, data_label)\n",
        "  # plot DI score\n",
        "  y_pos  = np.arange(len(di_scores))\n",
        "  plt.bar(y_pos, di_scores, align='center', alpha=0.5)\n",
        "  plt.xlabel('FilterNumber')\n",
        "  plt.ylabel('DI Score')\n",
        "\n",
        "  print('Mean of DI SCore: ',np.mean(di_scores))\n",
        "  print('STD of DI Score: ', np.std(di_scores))\n",
        "  print('MAX of DI Score: ', np.max(di_scores))\n",
        "#   plt.title('DI score - Data - D_L, Network -N_L ' + layer_name)\n",
        "  plt.title('DI score ' + layer_name)\n",
        "\n",
        "#   plt.ylim([0,1])\n",
        "  return di_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCCl7-vOfSZ9",
        "colab_type": "text"
      },
      "source": [
        "## load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqIkmDGkfRtD",
        "colab_type": "code",
        "outputId": "234726c4-ff08-43b5-b135-71b5c4ee588e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "loaded_model_low_accuracy = keras.models.load_model('vgg16lowAccuracyModelDataV3.h5')\n",
        "loaded_model_high_accuracy = keras.models.load_model('vgg16HighAccuracyModelData.h5')\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7J98O5qfi8Z",
        "colab_type": "code",
        "outputId": "959f3376-8077-4841-e5f9-05133b8f95ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loaded_model_low_accuracy.evaluate(valid_X, valid_ground, verbose=0)\n",
        "\n",
        "#[0.3284807046254476, 0.923]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4554041377703349, 0.8660000001589457]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rddbxdrtXcR7",
        "colab_type": "code",
        "outputId": "c59acc36-4b1f-45ab-cca9-11d07a71ed39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp_model = keras.models.load_model('vgg16lowAccuracyModelDataV3.h5')\n",
        "temp_model.evaluate(valid_X, valid_ground, verbose=0)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4554041436513265, 0.8660000001589457]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXuB6NbXmGB",
        "colab_type": "code",
        "outputId": "3cd3d5c2-7859-4f01-9582-7c9cf6fd70de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "valid_ground"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVSvyoZ8Uw_y",
        "colab_type": "code",
        "outputId": "d75d1de3-5678-4e46-a618-8ad8d4a04ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loaded_model_low_accuracy.evaluate(test_three_data, test_three_label_one_hot, verbose=0)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.45106768679618836, 0.8799999998410543]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAY0LGCNSMBc",
        "colab_type": "code",
        "outputId": "6759804b-f50c-4112-e5f8-5a81aa60042f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# loaded_model_low_accuracy.load_weights('weights.hdf5')\n",
        "# loaded_model_low_accuracy.evaluate(valid_X, valid_ground, verbose=0)\n",
        "loaded_model_high_accuracy.evaluate(valid_X, valid_ground, verbose=0)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.8258795127868654, 0.3953333332538605]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsm88PV3TlmL",
        "colab_type": "code",
        "outputId": "3e2269e7-2075-42f9-d04b-bebb054db442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loaded_model_high_accuracy.evaluate(test_three_data, test_three_label_one_hot, verbose=0)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.126735844930013, 0.3673333334128062]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-dQDj8vPnXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7c-A-a4SS6S",
        "colab_type": "code",
        "outputId": "96073616-b2d6-478b-91cf-6855de9b3e9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loaded_model_low_accuracy.evaluate(test_three_data, test_three_label_one_hot, verbose=0)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.45106768679618836, 0.8799999998410543]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzbIXvcEoNMj",
        "colab_type": "code",
        "outputId": "3a276e25-c69d-4284-8974-c0f3f78ba278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1101
        }
      },
      "source": [
        "loaded_model_low_accuracy.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1_1 (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "conv1_2 (BatchNormalization) (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "pool1_3 (MaxPooling2D)       (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2_1 (Conv2D)             (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2_2 (BatchNormalization) (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "pool2_3 (MaxPooling2D)       (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv3_1 (Conv2D)             (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv3_2 (BatchNormalization) (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv4_1 (Conv2D)             (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "conv4_2 (BatchNormalization) (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "pool4_2 (MaxPooling2D)       (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv5_1 (Conv2D)             (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv5_2 (BatchNormalization) (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv6_1 (Conv2D)             (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv6_2 (BatchNormalization) (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "pool6_2 (MaxPooling2D)       (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv7_1 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv7_2 (BatchNormalization) (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv8_1 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv8_2 (BatchNormalization) (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "pool8_2 (MaxPooling2D)       (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              2101248   \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "out (Dense)                  (None, 3)                 3003      \n",
            "=================================================================\n",
            "Total params: 32,214,051\n",
            "Trainable params: 32,208,547\n",
            "Non-trainable params: 5,504\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UijDrRL2khSY",
        "colab_type": "text"
      },
      "source": [
        "## Create a plot for DI score using function createCNNLayerDIScorePlot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyk2-GAGaPnV",
        "colab_type": "text"
      },
      "source": [
        "#### On train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN1yVJGzUzXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layer_name = 'conv2d_2'\n",
        "# model = loaded_model_low_accuracy\n",
        "# intermediate_layer_model = Model(i4nputs=model.input,\n",
        "#                                  outputs=model.get_layer(layer_name).output)\n",
        "# intermediate_output = intermediate_layer_model.predict(train_three_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_GyLE1oSbGn",
        "colab_type": "code",
        "outputId": "2fc54073-0805-4113-94d3-350ace01cc42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# now get DI score of a layer from high accuracy and low accuracy model\n",
        "# layer_names = ['conv2_1']\n",
        "layer_names = ['conv8_1']\n",
        "all_di_acores_low_accuracy = []\n",
        "for lay in layer_names:\n",
        "  this_di_score_low_accuracy_model = createCNNLayerDIScorePlot(loaded_model_low_accuracy, lay)\n",
        "  all_di_acores_low_accuracy.append(this_di_score_low_accuracy_model)\n",
        "# discores_low_acc_model = createCNNLayerDIScorePlot(loaded_model_low_accuracy, layer_name)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of DI SCore:  0.8694144080993281\n",
            "STD of DI Score:  0.6205526663901321\n",
            "MAX of DI Score:  1.5949378094135989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBUWcfwaYwsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# discores_low_acc_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siJntOdpYoNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.sort(discores_low_acc_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQC6i-0MVmWk",
        "colab_type": "code",
        "outputId": "59c1e27a-fe4b-405f-bfb2-e4a580fe8447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# now get DI score of a layer from high accuracy and low accuracy model\n",
        "#layer_name = 'conv2d_2'\n",
        "\n",
        "# discores_high_acc_model = createCNNLayerDIScorePlot(loaded_model_high_accuracy, layer_name)\n",
        "all_di_acores_high_accuracy = []\n",
        "for lay in layer_names:\n",
        "  this_di_score_high_accuracy_model = createCNNLayerDIScorePlot(loaded_model_high_accuracy, lay)\n",
        "  all_di_acores_high_accuracy.append(this_di_score_high_accuracy_model)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of DI SCore:  0.09890966478271793\n",
            "STD of DI Score:  0.08029777448636094\n",
            "MAX of DI Score:  0.2144610349284688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGjrlhMbVmUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHVud4ykeRn8",
        "colab_type": "text"
      },
      "source": [
        "## Pick top 10 DI scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k5Ob6gWeQ94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyI8gOZuftmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjMoET8nasck",
        "colab_type": "text"
      },
      "source": [
        "#### On validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPKQgmyXbACY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_ground_actual = np.argmax(valid_ground, axis=1)\n",
        "# valid_ground_actual.shape\n",
        "# data_to_use = train_three_data , data_label =train_three_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbg5-HKhaxxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now get DI score of a layer from high accuracy and low accuracy model\n",
        "#layer_name = 'conv2d_2'\n",
        "# discores_low_acc_model_valid = createCNNLayerDIScorePlot(loaded_model_low_accuracy,\n",
        "#                                                    layer_name, data_to_use=valid_X, data_label=valid_ground_actual)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaH4FbSFaxuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now get DI score of a layer from high accuracy and low accuracy model\n",
        "#layer_name = 'conv2d_2'\n",
        "\n",
        "# discores_high_acc_model_valid = createCNNLayerDIScorePlot(loaded_model_high_accuracy,\n",
        "#                                                    layer_name, data_to_use=valid_X, data_label=valid_ground_actual)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMpuiVdSf60g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.sort(discores_high_acc_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZueWb5hY7zZJ",
        "colab_type": "text"
      },
      "source": [
        "#### Pick top channels from the layers selected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9HwamF674hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topChoose = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmggM8xc8c8G",
        "colab_type": "code",
        "outputId": "f943c01b-882e-4bf4-d3e2-c042488542e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i,d in enumerate(all_di_acores_high_accuracy):\n",
        "  print(len(d),i)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1cAvZpx9G8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Ks = [128]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3zlCJ3R9jjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "copied_low_accuracy_model = loaded_model_low_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY28-KyEAhdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "copied_high_accuracy_model = loaded_model_high_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCeCjHCzbJj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# low_accuracy_model_weight.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgoNV1oG8BS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l,DIs in enumerate(all_di_acores_high_accuracy):\n",
        "  this_layer_name = layer_names[l]\n",
        "  this_high_accuracy_di_scores = all_di_acores_high_accuracy[l]\n",
        "  this_high_accuracy_di_scores_sorted = np.argsort(this_high_accuracy_di_scores)\n",
        "  this_high_accuracy_di_scores_sorted_list = np.ndarray.tolist(this_high_accuracy_di_scores_sorted)\n",
        "  top_channels = this_high_accuracy_di_scores_sorted_list[-Ks[l]:]\n",
        "  \n",
        "  # do same thing for lowest DIs\n",
        "  this_low_accuracy_di_scores = all_di_acores_low_accuracy[l]\n",
        "  this_low_accuracy_di_scores_sorted = np.argsort(this_low_accuracy_di_scores)\n",
        "  this_low_accuracy_di_scores_sorted_list = np.ndarray.tolist(this_low_accuracy_di_scores_sorted)\n",
        "  bottom_channels = this_low_accuracy_di_scores_sorted_list[0:Ks[l]]\n",
        "  \n",
        "  # now replace the weights\n",
        "  low_accuracy_model_weights_bias = copied_low_accuracy_model.get_layer(this_layer_name).get_weights()\n",
        "  high_accuracy_model_weights_bias = copied_high_accuracy_model.get_layer(this_layer_name).get_weights()\n",
        "\n",
        "  low_accuracy_model_weight,low_accuracy_model_bias = low_accuracy_model_weights_bias[0],low_accuracy_model_weights_bias[1]\n",
        "  high_accuracy_model_weight,high_accuracy_model_bias = high_accuracy_model_weights_bias[0],high_accuracy_model_weights_bias[1]\n",
        "#   print(low_accuracy_model_weight.shape,low_accuracy_model_bias.shape)\n",
        "  low_accuracy_model_weight[:,:,:,bottom_channels] = high_accuracy_model_weight[:,:,:,top_channels]\n",
        "#   print(bottom_channels)\n",
        "  high_accuracy_model_bias[bottom_channels] = high_accuracy_model_bias[top_channels]\n",
        "  \n",
        "  copied_low_accuracy_model.get_layer(this_layer_name).set_weights([low_accuracy_model_weight, high_accuracy_model_bias])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OodsSlYDCesc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# high_accuracy_model_weights_bias.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AtLgL9ICGJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# high_accuracy_model_bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzN_W73aldFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# # (discores_high_acc_model)\n",
        "# K= 50 # how many channels to pick\n",
        "# high_acc_di_sorted = np.argsort(discores_high_acc_model)\n",
        "# high_acc_di_sorted_list = np.ndarray.tolist(high_acc_di_sorted)\n",
        "# top_channels = high_acc_di_sorted_list[-K:]#np.random.choice(32,K)#\n",
        "# print('Top Channels: ',top_channels)\n",
        "\n",
        "\n",
        "# low_acc_di_sorted = np.argsort(discores_low_acc_model)\n",
        "# low_acc_di_sorted_list = np.ndarray.tolist(low_acc_di_sorted)\n",
        "# low_channels = low_acc_di_sorted_list[0:K]#np.random.choice(32,K)\n",
        "# print('Low Channels: ', low_channels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_BDN-T-BaGz",
        "colab_type": "text"
      },
      "source": [
        "#####  get low accuracy DI channels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5DgBrzXCGB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# discores_low_acc_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5B34pIwCbXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.plot(discores_low_acc_model,'-*')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRrtIpLVBdK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD8nswMnxWVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# total_channels = len(discores_high_acc_model)\n",
        "# total_channels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsCtxzRVokuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enable_training = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk_0a6JlFux5",
        "colab_type": "code",
        "outputId": "9d55385b-87f3-4340-eb50-592b28eb6a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# returns \n",
        "'''\n",
        "def highAccuracyModelTillConv2(input_img): \n",
        "  conv1_1 = (Conv2D(32, (3,3), padding='same',\n",
        "                    kernel_regularizer=regularizers.l2(weight_decay), \n",
        "                    input_shape=x_train.shape[1:], activation='elu', trainable = enable_training,\n",
        "                    name = 'HighAccuracyConv1'))(input_img)\n",
        "  conv1_2 = BatchNormalization(name = 'HighAccuracyBN1',\n",
        "                              trainable = enable_training)(conv1_1)\n",
        "  conv2_1 = (Conv2D(32, (3,3), padding='same', activation= 'elu',trainable = enable_training,\n",
        "                    name = 'HighAccuracyConv2',kernel_regularizer=regularizers.l2(weight_decay)))(conv1_2)\n",
        "#   conv2_2 = BatchNormalization(name = 'HighAccuracyBN2',\n",
        "#                               trainable = enable_training)(conv2_1)  \n",
        "  \n",
        "  return conv2_1\n",
        "'''"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef highAccuracyModelTillConv2(input_img): \\n  conv1_1 = (Conv2D(32, (3,3), padding='same',\\n                    kernel_regularizer=regularizers.l2(weight_decay), \\n                    input_shape=x_train.shape[1:], activation='elu', trainable = enable_training,\\n                    name = 'HighAccuracyConv1'))(input_img)\\n  conv1_2 = BatchNormalization(name = 'HighAccuracyBN1',\\n                              trainable = enable_training)(conv1_1)\\n  conv2_1 = (Conv2D(32, (3,3), padding='same', activation= 'elu',trainable = enable_training,\\n                    name = 'HighAccuracyConv2',kernel_regularizer=regularizers.l2(weight_decay)))(conv1_2)\\n#   conv2_2 = BatchNormalization(name = 'HighAccuracyBN2',\\n#                               trainable = enable_training)(conv2_1)  \\n  \\n  return conv2_1\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZchwmWh7la9",
        "colab_type": "code",
        "outputId": "45ed05a2-ce4a-4d78-a44f-3a96fbe97e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1101
        }
      },
      "source": [
        "copied_low_accuracy_model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1_1 (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "conv1_2 (BatchNormalization) (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "pool1_3 (MaxPooling2D)       (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2_1 (Conv2D)             (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2_2 (BatchNormalization) (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "pool2_3 (MaxPooling2D)       (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv3_1 (Conv2D)             (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv3_2 (BatchNormalization) (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv4_1 (Conv2D)             (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "conv4_2 (BatchNormalization) (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "pool4_2 (MaxPooling2D)       (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv5_1 (Conv2D)             (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv5_2 (BatchNormalization) (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv6_1 (Conv2D)             (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv6_2 (BatchNormalization) (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "pool6_2 (MaxPooling2D)       (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv7_1 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv7_2 (BatchNormalization) (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv8_1 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv8_2 (BatchNormalization) (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "pool8_2 (MaxPooling2D)       (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              2101248   \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "out (Dense)                  (None, 3)                 3003      \n",
            "=================================================================\n",
            "Total params: 32,214,051\n",
            "Trainable params: 32,208,547\n",
            "Non-trainable params: 5,504\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA-0pVGXvMeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Concatenate, Input, Lambda\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41smOd7GTX5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def cifar10ClassifierTransfer(input_img):\n",
        "  \n",
        "#   conv1_1 = (Conv2D(64, (3,3),name = 'conv1_1' ,padding='same', \n",
        "#                     kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:],\n",
        "#                     activation='relu'))(input_img)\n",
        "#   conv1_2 = BatchNormalization(name = 'conv1_2')(conv1_1)\n",
        "#   pool1_3 = MaxPooling2D(pool_size=(2, 2), name = 'pool1_3')(conv1_2) #14 x 14 x 32\n",
        "#   conv2_1 = (Conv2D(128, (3,3),name = 'conv2_1' ,padding='same',\n",
        "#                     activation= 'relu',kernel_regularizer=regularizers.l2(weight_decay)))(pool1_3)\n",
        "#   conv2_2 = BatchNormalization(name = 'conv2_2')(conv2_1)\n",
        "#   pool2_3 = MaxPooling2D(pool_size=(2, 2), name = 'pool2_3')(conv2_2) #14 x 14 x 32\n",
        "  \n",
        "#   conv3_1 = (Conv2D(256, (3,3), padding='same',name = 'conv3_1',\n",
        "#                     kernel_regularizer=regularizers.l2(weight_decay), activation='relu'))(pool2_3)\n",
        "#   conv3_2 = BatchNormalization(name = 'conv3_2')(conv3_1)\n",
        "#   conv4_1 = (Conv2D(256, (3,3), padding='same',name = 'conv4_1' ,activation= 'relu',\n",
        "#                     kernel_regularizer=regularizers.l2(weight_decay)))(conv3_2)\n",
        "#   conv4_2 = BatchNormalization(name = 'conv4_2')(conv4_1)\n",
        "#   pool4_2 = MaxPooling2D(pool_size=(2, 2), name = 'pool4_2')(conv4_2) #14 x 14 x 32\n",
        "  \n",
        "#   conv5_1 = (Conv2D(512, (3,3), padding='same',name = 'conv5_1',\n",
        "#                     kernel_regularizer=regularizers.l2(weight_decay), activation='relu'))(pool4_2)\n",
        "#   conv5_2 = BatchNormalization(name = 'conv5_2')(conv5_1)\n",
        "#   conv6_1 = (Conv2D(512, (3,3), padding='same',name = 'conv6_1' ,activation= 'relu',\n",
        "#                     kernel_regularizer=regularizers.l2(weight_decay)))(conv5_2)\n",
        "#   conv6_2 = BatchNormalization(name = 'conv6_2')(conv6_1)\n",
        "#   pool6_2 = MaxPooling2D(pool_size=(2, 2), name = 'pool6_2')(conv6_2) #14 x 14 x 32\n",
        "  \n",
        "#   conv7_1 = (Conv2D(512, (3,3), padding='same',name = 'conv7_1',\n",
        "#                     kernel_regularizer=regularizers.l2(weight_decay), activation='relu'))(pool6_2)\n",
        "#   conv7_2 = BatchNormalization(name = 'conv7_2')(conv7_1)\n",
        "#   conv8_1 = (Conv2D(512, (3,3), padding='same',name = 'conv8_1' ,activation= 'relu',\n",
        "#                     kernel_regularizer=regularizers.l2(weight_decay)))(conv7_2)\n",
        "#   conv8_2 = BatchNormalization(name = 'conv8_2')(conv8_1)\n",
        "#   pool8_2 = MaxPooling2D(pool_size=(2, 2), name = 'pool8_2')(conv8_2) #14 x 14 x 32\n",
        "\n",
        "\n",
        "#   flat = Flatten()(pool8_2)\n",
        "#   fc1 = Dense(4096, activation='relu', name = 'fc1')(flat)\n",
        "#   fc2 = Dense(4096, activation='relu', name = 'fc2')(fc1)\n",
        "#   fc3 = Dense(1000, activation='relu', name = 'fc3')(fc2)\n",
        "\n",
        "\n",
        "  \n",
        "#   out = Dense(num_classes, activation='softmax', name = 'out')(fc3)\n",
        "  \n",
        "#   return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf3PN5YIyba2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# top_channels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzUmKtVGFusU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #high_acc_output = highAccuracyModelTillConv2(input_img)\n",
        "# modelFilt = Model(input_img, \n",
        "#               cifar10ClassifierTransfer(input_img))\n",
        "# #data augmentation\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=15,\n",
        "#     width_shift_range=0.1,\n",
        "#     height_shift_range=0.1,\n",
        "#     horizontal_flip=True,\n",
        "#     )\n",
        "# datagen.fit(train_three_data)\n",
        " \n",
        "# #training\n",
        "# batch_size = 64\n",
        " \n",
        "# opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
        "# # model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "\n",
        "# modelFilt.compile(loss=losses.categorical_crossentropy, optimizer=opt_rms, metrics=['accuracy'])\n",
        "\n",
        "# modelFilt.summary()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cty76ahJDRhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(train_three_data)\n",
        "# #training\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE7b72OOFups",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(copied_low_accuracy_model, to_file=\"FiltModel.png\", show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-yqdp6_rQkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loaded_model_high_accuracy.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GkUnIIAqH0k",
        "colab_type": "text"
      },
      "source": [
        "## set predetermined weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKVw7k8QLwoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# top_channels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hWcRyTNtaXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# top_channels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhmWPU7RA9N3",
        "colab_type": "text"
      },
      "source": [
        "#### # get low accuracy DI channel - stored in low_channels\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL5R24pGA9KP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0JBUK8KA26B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# loaded_model_high_accuracy.get_layer(layer_name).get_weights()[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmS9J6gmDFIL",
        "colab_type": "text"
      },
      "source": [
        "## Get filter weights for top K10 DI scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0FN66utr5VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # set weights of high accuracy model for channels not used to 0 [9, 25, 19, 29, 5, 7, 26, 28, 17, 23]\n",
        "# high_acc_conv2_weights = loaded_model_high_accuracy.get_layer(layer_name).get_weights()\n",
        "# # temp_weights = np.zeros_like(high_acc_conv2_weights[0].shape[0])\n",
        "# temp_weights_top_10_DI = np.zeros(shape = (high_acc_conv2_weights[0].shape[0],high_acc_conv2_weights[0].shape[1],high_acc_conv2_weights[0].shape[2],K))\n",
        "# h,w,d,n = high_acc_conv2_weights[0].shape\n",
        "# j = 0\n",
        "# for ch in range(0,n):\n",
        "#   if ch in top_channels:\n",
        "#     temp_weights_top_10_DI[:,:,:,j] = high_acc_conv2_weights[0][:,:,:,ch]\n",
        "#     j = j+1\n",
        "    \n",
        "  \n",
        "# # (high_acc_conv2_weights[0].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF9DJY3BD6Wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# temp_weights_top_10_DI.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lR-sOsQD6UV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# top_channels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esv5wQmYFby6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# temp_bias_top_10_DI = np.zeros(shape = (K,))\n",
        "# # temp_bias = high_acc_conv2_weights[1]\n",
        "\n",
        "# # bias_shape = high_acc_conv2_weights[1].shape\n",
        "# j = 0\n",
        "# for ch in range(0,high_acc_conv2_weights[1].shape[0]):\n",
        "# #   print (ch)\n",
        "#   if ch in top_channels:\n",
        "#     temp_bias_top_10_DI[j] = high_acc_conv2_weights[1][ch]\n",
        "#     j = j+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPV28w1jFulD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# temp_bias\n",
        "# weights_to_assign_fromHighAcc = [temp_weights_top_10_DI,temp_bias_top_10_DI]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pYigm1qFJmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# temp_weights_top_10_DI.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1t2Yj3-E01G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# high_acc_conv2_weights[1].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQKKEkR2F-tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modelFilt.get_layer('lowAccuracyModelConv1_1').set_weights(loaded_model_low_accuracy.get_layer('conv2d_1').get_weights())\n",
        "# modelFilt.get_layer('lowAccuracyModelBN1').set_weights(loaded_model_low_accuracy.get_layer('batch_normalization_1').get_weights())\n",
        "# # modelFilt.get_layer('HighAccuracyConv2').set_weights(loaded_model_high_accuracy.get_layer('conv2d_2').get_weights())\n",
        "# modelFilt.get_layer('lowAccuracyModelConv2_1').set_weights(loaded_model_low_accuracy.get_layer('conv2d_2').get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knA2xBI4jrZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loaded_model_low_accuracy.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ9nBYl-h9SD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # preset the weights\n",
        "# for l1,l2 in zip(modelFilt.layers,loaded_model_low_accuracy.layers):\n",
        "#     l1.set_weights(l2.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGIUVWh3G4x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layer_to_replace = layer_name\n",
        "# low_acc_conv2d_weights_bias = modelFilt.get_layer(layer_to_replace).get_weights()\n",
        "# low_acc_conv2d_weights = low_acc_conv2d_weights_bias[0]\n",
        "# low_acc_conv2d_wbias= low_acc_conv2d_weights_bias[1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVFju9iFKJID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# temp_bias_top_10_DI.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlkRaotsHceY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # now change weights for layers corresponding to lowest DI scores\n",
        "# orig_weights = np.copy(low_acc_conv2d_weights)\n",
        "# j = 0\n",
        "# for ch in range(0,orig_weights.shape[3]):\n",
        "#   if ch in low_channels:\n",
        "# #     print(ch)\n",
        "#     orig_weights[:,:,:,ch] = temp_weights_top_10_DI[:,:,:,j]\n",
        "#     j = j+1\n",
        "# # orig_weights.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye2_DU60K0nL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# orig_bias = np.copy(low_acc_conv2d_wbias)\n",
        "# j = 0\n",
        "# for ch in range(0,orig_bias.shape[0]):\n",
        "#   if ch in low_channels:\n",
        "# #     print(ch)\n",
        "#     orig_bias[ch] = temp_bias_top_10_DI[j]\n",
        "#     j = j+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDX113kegLSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RERDrTNALkUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new_wt_bias_conv2d1 = [orig_weights,orig_bias]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMc3GS52iEv0",
        "colab_type": "text"
      },
      "source": [
        "#### Overwrite weights fow low DI with high DI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTUKj97VLfIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modelFilt.get_layer(layer_to_replace).set_weights(new_wt_bias_conv2d1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji5jVmbej-LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loaded_model_low_accuracy.layers[5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w2grlRlKQhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EozUm-dSIx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modelFilt.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRWAVstvGhht",
        "colab_type": "text"
      },
      "source": [
        "#### # now replace weights in conv2d layer for filters correspondng to lowest 10 DI scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcnUQrPtGay_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fev8cENxbr-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_save_name = 'weights_top_k_model2.hdf5'\n",
        "mcp_save = ModelCheckpoint(file_save_name, save_best_only=True, monitor='val_acc', mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BTKJa7Zbwtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lrate = 0.0001\n",
        "    if epoch > 300:\n",
        "        lrate = 0.0001/3\n",
        "    if epoch > 600:\n",
        "        lrate = 0.0001/9\n",
        "    return lrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdiZwWSth97S",
        "colab_type": "code",
        "outputId": "9302212d-1560-4e12-92a5-526f0cf6e0f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "copied_low_accuracy_model.evaluate(valid_X, valid_ground, verbose=0)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11.013150327046713, 0.3246666665871938]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIOMXQsjdSNX",
        "colab_type": "code",
        "outputId": "3ac8a955-1ba3-4d17-afe6-cca155b8664d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21903
        }
      },
      "source": [
        "copied_low_accuracy_model.fit_generator(datagen.flow(train_X, train_ground, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=train_X.shape[0] // batch_size,epochs=1000,\\\n",
        "                    verbose=1,validation_data=(valid_X,valid_ground),\n",
        "                    shuffle = True,callbacks=[mcp_save,LearningRateScheduler(lr_schedule)])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "187/187 [==============================] - 13s 68ms/step - loss: 0.3136 - acc: 0.9301 - val_loss: 0.4962 - val_acc: 0.8767\n",
            "Epoch 2/1000\n",
            "187/187 [==============================] - 10s 51ms/step - loss: 0.2815 - acc: 0.9404 - val_loss: 0.4792 - val_acc: 0.8787\n",
            "Epoch 3/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2750 - acc: 0.9418 - val_loss: 1.0199 - val_acc: 0.8287\n",
            "Epoch 4/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.2631 - acc: 0.9463 - val_loss: 0.4883 - val_acc: 0.8787\n",
            "Epoch 5/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2632 - acc: 0.9458 - val_loss: 0.4786 - val_acc: 0.8767\n",
            "Epoch 6/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2484 - acc: 0.9519 - val_loss: 0.4724 - val_acc: 0.8807\n",
            "Epoch 7/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2460 - acc: 0.9531 - val_loss: 0.5142 - val_acc: 0.8830\n",
            "Epoch 8/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2386 - acc: 0.9550 - val_loss: 0.5549 - val_acc: 0.8800\n",
            "Epoch 9/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2408 - acc: 0.9536 - val_loss: 0.4996 - val_acc: 0.8843\n",
            "Epoch 10/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2305 - acc: 0.9586 - val_loss: 0.5392 - val_acc: 0.8850\n",
            "Epoch 11/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2258 - acc: 0.9594 - val_loss: 0.4840 - val_acc: 0.8887\n",
            "Epoch 12/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2315 - acc: 0.9569 - val_loss: 0.4873 - val_acc: 0.8853\n",
            "Epoch 13/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.2304 - acc: 0.9593 - val_loss: 0.4823 - val_acc: 0.8877\n",
            "Epoch 14/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.2180 - acc: 0.9604 - val_loss: 0.5103 - val_acc: 0.8820\n",
            "Epoch 15/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2202 - acc: 0.9596 - val_loss: 0.5460 - val_acc: 0.8847\n",
            "Epoch 16/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2194 - acc: 0.9606 - val_loss: 0.5153 - val_acc: 0.8897\n",
            "Epoch 17/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2044 - acc: 0.9651 - val_loss: 0.5438 - val_acc: 0.8863\n",
            "Epoch 18/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2095 - acc: 0.9647 - val_loss: 0.5688 - val_acc: 0.8797\n",
            "Epoch 19/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2097 - acc: 0.9644 - val_loss: 0.5361 - val_acc: 0.8823\n",
            "Epoch 20/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2092 - acc: 0.9621 - val_loss: 0.5639 - val_acc: 0.8777\n",
            "Epoch 21/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.2043 - acc: 0.9654 - val_loss: 0.5753 - val_acc: 0.8837\n",
            "Epoch 22/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.2052 - acc: 0.9659 - val_loss: 0.5181 - val_acc: 0.8930\n",
            "Epoch 23/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1992 - acc: 0.9688 - val_loss: 0.5335 - val_acc: 0.8870\n",
            "Epoch 24/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1970 - acc: 0.9681 - val_loss: 0.5467 - val_acc: 0.8917\n",
            "Epoch 25/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1946 - acc: 0.9708 - val_loss: 0.5222 - val_acc: 0.8850\n",
            "Epoch 26/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1949 - acc: 0.9680 - val_loss: 0.5580 - val_acc: 0.8820\n",
            "Epoch 27/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1854 - acc: 0.9717 - val_loss: 0.5236 - val_acc: 0.8837\n",
            "Epoch 28/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1838 - acc: 0.9722 - val_loss: 0.5819 - val_acc: 0.8893\n",
            "Epoch 29/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1855 - acc: 0.9732 - val_loss: 0.5547 - val_acc: 0.8863\n",
            "Epoch 30/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1843 - acc: 0.9730 - val_loss: 0.5798 - val_acc: 0.8900\n",
            "Epoch 31/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1835 - acc: 0.9720 - val_loss: 0.5912 - val_acc: 0.8947\n",
            "Epoch 32/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1818 - acc: 0.9707 - val_loss: 0.5463 - val_acc: 0.8883\n",
            "Epoch 33/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1862 - acc: 0.9733 - val_loss: 0.5718 - val_acc: 0.8867\n",
            "Epoch 34/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1781 - acc: 0.9749 - val_loss: 0.5287 - val_acc: 0.8880\n",
            "Epoch 35/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1735 - acc: 0.9753 - val_loss: 0.6002 - val_acc: 0.8830\n",
            "Epoch 36/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1739 - acc: 0.9743 - val_loss: 0.5294 - val_acc: 0.8903\n",
            "Epoch 37/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1656 - acc: 0.9760 - val_loss: 0.6948 - val_acc: 0.8820\n",
            "Epoch 38/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1774 - acc: 0.9748 - val_loss: 0.5711 - val_acc: 0.8837\n",
            "Epoch 39/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1747 - acc: 0.9759 - val_loss: 0.5706 - val_acc: 0.8873\n",
            "Epoch 40/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1670 - acc: 0.9770 - val_loss: 0.6438 - val_acc: 0.8863\n",
            "Epoch 41/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1682 - acc: 0.9777 - val_loss: 0.5818 - val_acc: 0.8930\n",
            "Epoch 42/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1634 - acc: 0.9790 - val_loss: 0.6748 - val_acc: 0.8943\n",
            "Epoch 43/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1686 - acc: 0.9769 - val_loss: 0.5835 - val_acc: 0.8903\n",
            "Epoch 44/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1632 - acc: 0.9796 - val_loss: 0.5805 - val_acc: 0.8923\n",
            "Epoch 45/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1580 - acc: 0.9807 - val_loss: 0.5737 - val_acc: 0.8827\n",
            "Epoch 46/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1651 - acc: 0.9776 - val_loss: 0.5800 - val_acc: 0.8870\n",
            "Epoch 47/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1560 - acc: 0.9806 - val_loss: 0.6320 - val_acc: 0.8860\n",
            "Epoch 48/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1604 - acc: 0.9795 - val_loss: 0.5514 - val_acc: 0.8893\n",
            "Epoch 49/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1573 - acc: 0.9803 - val_loss: 0.6656 - val_acc: 0.8837\n",
            "Epoch 50/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1560 - acc: 0.9812 - val_loss: 0.6978 - val_acc: 0.8830\n",
            "Epoch 51/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1575 - acc: 0.9804 - val_loss: 0.7033 - val_acc: 0.8827\n",
            "Epoch 52/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1543 - acc: 0.9820 - val_loss: 0.6140 - val_acc: 0.8863\n",
            "Epoch 53/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1487 - acc: 0.9825 - val_loss: 0.6780 - val_acc: 0.8867\n",
            "Epoch 54/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1553 - acc: 0.9798 - val_loss: 0.6826 - val_acc: 0.8887\n",
            "Epoch 55/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1467 - acc: 0.9839 - val_loss: 0.7504 - val_acc: 0.8843\n",
            "Epoch 56/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1482 - acc: 0.9840 - val_loss: 0.6800 - val_acc: 0.8907\n",
            "Epoch 57/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1507 - acc: 0.9826 - val_loss: 0.6672 - val_acc: 0.8910\n",
            "Epoch 58/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1525 - acc: 0.9826 - val_loss: 0.5766 - val_acc: 0.8890\n",
            "Epoch 59/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1501 - acc: 0.9833 - val_loss: 0.6114 - val_acc: 0.8883\n",
            "Epoch 60/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1414 - acc: 0.9837 - val_loss: 0.7286 - val_acc: 0.8847\n",
            "Epoch 61/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1445 - acc: 0.9837 - val_loss: 0.7330 - val_acc: 0.8827\n",
            "Epoch 62/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1423 - acc: 0.9844 - val_loss: 0.6899 - val_acc: 0.8893\n",
            "Epoch 63/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1490 - acc: 0.9816 - val_loss: 0.7182 - val_acc: 0.8857\n",
            "Epoch 64/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1406 - acc: 0.9852 - val_loss: 0.7096 - val_acc: 0.8857\n",
            "Epoch 65/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1419 - acc: 0.9853 - val_loss: 0.6021 - val_acc: 0.8907\n",
            "Epoch 66/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1416 - acc: 0.9845 - val_loss: 0.6743 - val_acc: 0.8843\n",
            "Epoch 67/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1413 - acc: 0.9860 - val_loss: 0.7351 - val_acc: 0.8813\n",
            "Epoch 68/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1382 - acc: 0.9855 - val_loss: 0.7235 - val_acc: 0.8907\n",
            "Epoch 69/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1368 - acc: 0.9852 - val_loss: 0.6966 - val_acc: 0.8790\n",
            "Epoch 70/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1400 - acc: 0.9858 - val_loss: 0.7432 - val_acc: 0.8843\n",
            "Epoch 71/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1397 - acc: 0.9864 - val_loss: 0.6844 - val_acc: 0.8890\n",
            "Epoch 72/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1379 - acc: 0.9857 - val_loss: 0.7373 - val_acc: 0.8863\n",
            "Epoch 73/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1361 - acc: 0.9867 - val_loss: 0.7719 - val_acc: 0.8860\n",
            "Epoch 74/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1380 - acc: 0.9865 - val_loss: 0.6466 - val_acc: 0.8867\n",
            "Epoch 75/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1366 - acc: 0.9859 - val_loss: 0.7480 - val_acc: 0.8917\n",
            "Epoch 76/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1407 - acc: 0.9852 - val_loss: 0.6583 - val_acc: 0.8940\n",
            "Epoch 77/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1356 - acc: 0.9870 - val_loss: 0.8445 - val_acc: 0.8850\n",
            "Epoch 78/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1330 - acc: 0.9868 - val_loss: 0.7442 - val_acc: 0.8893\n",
            "Epoch 79/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1300 - acc: 0.9874 - val_loss: 0.6702 - val_acc: 0.8877\n",
            "Epoch 80/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1378 - acc: 0.9866 - val_loss: 0.8110 - val_acc: 0.8833\n",
            "Epoch 81/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1377 - acc: 0.9871 - val_loss: 0.7806 - val_acc: 0.8850\n",
            "Epoch 82/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1395 - acc: 0.9867 - val_loss: 0.7245 - val_acc: 0.8860\n",
            "Epoch 83/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1302 - acc: 0.9878 - val_loss: 0.7847 - val_acc: 0.8847\n",
            "Epoch 84/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1336 - acc: 0.9881 - val_loss: 0.8079 - val_acc: 0.8863\n",
            "Epoch 85/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1353 - acc: 0.9875 - val_loss: 0.6516 - val_acc: 0.8900\n",
            "Epoch 86/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1367 - acc: 0.9867 - val_loss: 0.8389 - val_acc: 0.8917\n",
            "Epoch 87/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1309 - acc: 0.9883 - val_loss: 0.6929 - val_acc: 0.8890\n",
            "Epoch 88/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1305 - acc: 0.9864 - val_loss: 0.6601 - val_acc: 0.8850\n",
            "Epoch 89/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1280 - acc: 0.9887 - val_loss: 0.6884 - val_acc: 0.8900\n",
            "Epoch 90/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1300 - acc: 0.9870 - val_loss: 0.7594 - val_acc: 0.8900\n",
            "Epoch 91/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1283 - acc: 0.9891 - val_loss: 0.6601 - val_acc: 0.8907\n",
            "Epoch 92/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1240 - acc: 0.9893 - val_loss: 0.6379 - val_acc: 0.8893\n",
            "Epoch 93/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1243 - acc: 0.9892 - val_loss: 0.7150 - val_acc: 0.8873\n",
            "Epoch 94/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1254 - acc: 0.9897 - val_loss: 0.6776 - val_acc: 0.8863\n",
            "Epoch 95/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1254 - acc: 0.9889 - val_loss: 0.8668 - val_acc: 0.8890\n",
            "Epoch 96/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1245 - acc: 0.9886 - val_loss: 0.8084 - val_acc: 0.8847\n",
            "Epoch 97/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1324 - acc: 0.9877 - val_loss: 0.7463 - val_acc: 0.8817\n",
            "Epoch 98/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1291 - acc: 0.9881 - val_loss: 0.7077 - val_acc: 0.8907\n",
            "Epoch 99/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1269 - acc: 0.9897 - val_loss: 0.7854 - val_acc: 0.8810\n",
            "Epoch 100/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1266 - acc: 0.9897 - val_loss: 0.6920 - val_acc: 0.8860\n",
            "Epoch 101/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1273 - acc: 0.9875 - val_loss: 0.8409 - val_acc: 0.8833\n",
            "Epoch 102/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1303 - acc: 0.9878 - val_loss: 0.6405 - val_acc: 0.8890\n",
            "Epoch 103/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1211 - acc: 0.9902 - val_loss: 0.7741 - val_acc: 0.8920\n",
            "Epoch 104/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1232 - acc: 0.9893 - val_loss: 0.7431 - val_acc: 0.8857\n",
            "Epoch 105/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1260 - acc: 0.9891 - val_loss: 0.7937 - val_acc: 0.8807\n",
            "Epoch 106/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1203 - acc: 0.9904 - val_loss: 0.7593 - val_acc: 0.8850\n",
            "Epoch 107/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1227 - acc: 0.9907 - val_loss: 0.7972 - val_acc: 0.8903\n",
            "Epoch 108/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1194 - acc: 0.9904 - val_loss: 0.7770 - val_acc: 0.8730\n",
            "Epoch 109/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1215 - acc: 0.9897 - val_loss: 0.9164 - val_acc: 0.8860\n",
            "Epoch 110/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1244 - acc: 0.9908 - val_loss: 0.9056 - val_acc: 0.8880\n",
            "Epoch 111/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1299 - acc: 0.9890 - val_loss: 0.7796 - val_acc: 0.8870\n",
            "Epoch 112/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1183 - acc: 0.9903 - val_loss: 0.9511 - val_acc: 0.8830\n",
            "Epoch 113/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1222 - acc: 0.9905 - val_loss: 0.7553 - val_acc: 0.8880\n",
            "Epoch 114/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1202 - acc: 0.9888 - val_loss: 0.6994 - val_acc: 0.8883\n",
            "Epoch 115/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1234 - acc: 0.9908 - val_loss: 0.7901 - val_acc: 0.8860\n",
            "Epoch 116/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1126 - acc: 0.9915 - val_loss: 0.9087 - val_acc: 0.8750\n",
            "Epoch 117/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1255 - acc: 0.9891 - val_loss: 0.7974 - val_acc: 0.8880\n",
            "Epoch 118/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1242 - acc: 0.9894 - val_loss: 0.7395 - val_acc: 0.8893\n",
            "Epoch 119/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1183 - acc: 0.9896 - val_loss: 0.8629 - val_acc: 0.8917\n",
            "Epoch 120/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1197 - acc: 0.9903 - val_loss: 0.7937 - val_acc: 0.8883\n",
            "Epoch 121/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1199 - acc: 0.9908 - val_loss: 0.7230 - val_acc: 0.8880\n",
            "Epoch 122/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1183 - acc: 0.9909 - val_loss: 0.8471 - val_acc: 0.8867\n",
            "Epoch 123/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1163 - acc: 0.9914 - val_loss: 0.8158 - val_acc: 0.8863\n",
            "Epoch 124/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1189 - acc: 0.9900 - val_loss: 0.7826 - val_acc: 0.8950\n",
            "Epoch 125/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1158 - acc: 0.9906 - val_loss: 0.9488 - val_acc: 0.8933\n",
            "Epoch 126/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1152 - acc: 0.9923 - val_loss: 0.9139 - val_acc: 0.8913\n",
            "Epoch 127/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1166 - acc: 0.9911 - val_loss: 0.8550 - val_acc: 0.8903\n",
            "Epoch 128/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1130 - acc: 0.9917 - val_loss: 1.0289 - val_acc: 0.8887\n",
            "Epoch 129/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1156 - acc: 0.9916 - val_loss: 0.9141 - val_acc: 0.8890\n",
            "Epoch 130/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1283 - acc: 0.9908 - val_loss: 0.8028 - val_acc: 0.8913\n",
            "Epoch 131/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1175 - acc: 0.9915 - val_loss: 0.8046 - val_acc: 0.8897\n",
            "Epoch 132/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1170 - acc: 0.9903 - val_loss: 0.8183 - val_acc: 0.8917\n",
            "Epoch 133/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1214 - acc: 0.9901 - val_loss: 0.9068 - val_acc: 0.8890\n",
            "Epoch 134/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1177 - acc: 0.9916 - val_loss: 0.8503 - val_acc: 0.8887\n",
            "Epoch 135/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1157 - acc: 0.9906 - val_loss: 0.8214 - val_acc: 0.8920\n",
            "Epoch 136/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1130 - acc: 0.9901 - val_loss: 0.8193 - val_acc: 0.8887\n",
            "Epoch 137/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1183 - acc: 0.9903 - val_loss: 0.9268 - val_acc: 0.8950\n",
            "Epoch 138/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1164 - acc: 0.9912 - val_loss: 0.8240 - val_acc: 0.8843\n",
            "Epoch 139/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1078 - acc: 0.9926 - val_loss: 0.8855 - val_acc: 0.8933\n",
            "Epoch 140/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1090 - acc: 0.9931 - val_loss: 0.9603 - val_acc: 0.8850\n",
            "Epoch 141/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1196 - acc: 0.9908 - val_loss: 0.8251 - val_acc: 0.8833\n",
            "Epoch 142/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1152 - acc: 0.9912 - val_loss: 0.7596 - val_acc: 0.8820\n",
            "Epoch 143/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1179 - acc: 0.9907 - val_loss: 0.8738 - val_acc: 0.8880\n",
            "Epoch 144/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1192 - acc: 0.9901 - val_loss: 0.9104 - val_acc: 0.8837\n",
            "Epoch 145/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1111 - acc: 0.9922 - val_loss: 0.7973 - val_acc: 0.8903\n",
            "Epoch 146/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1132 - acc: 0.9906 - val_loss: 0.7275 - val_acc: 0.8897\n",
            "Epoch 147/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1085 - acc: 0.9920 - val_loss: 0.9311 - val_acc: 0.8903\n",
            "Epoch 148/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1121 - acc: 0.9916 - val_loss: 0.8614 - val_acc: 0.8857\n",
            "Epoch 149/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1128 - acc: 0.9918 - val_loss: 0.7398 - val_acc: 0.8887\n",
            "Epoch 150/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1121 - acc: 0.9923 - val_loss: 0.7813 - val_acc: 0.8943\n",
            "Epoch 151/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1145 - acc: 0.9911 - val_loss: 0.8886 - val_acc: 0.8920\n",
            "Epoch 152/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1149 - acc: 0.9912 - val_loss: 0.7911 - val_acc: 0.8910\n",
            "Epoch 153/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1099 - acc: 0.9916 - val_loss: 0.8888 - val_acc: 0.8827\n",
            "Epoch 154/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1171 - acc: 0.9898 - val_loss: 0.7471 - val_acc: 0.8853\n",
            "Epoch 155/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1121 - acc: 0.9901 - val_loss: 0.8315 - val_acc: 0.8877\n",
            "Epoch 156/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1109 - acc: 0.9911 - val_loss: 0.8530 - val_acc: 0.8933\n",
            "Epoch 157/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1124 - acc: 0.9928 - val_loss: 0.8741 - val_acc: 0.8840\n",
            "Epoch 158/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1100 - acc: 0.9913 - val_loss: 0.8164 - val_acc: 0.8907\n",
            "Epoch 159/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1077 - acc: 0.9919 - val_loss: 0.8205 - val_acc: 0.8897\n",
            "Epoch 160/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1107 - acc: 0.9916 - val_loss: 0.7852 - val_acc: 0.8937\n",
            "Epoch 161/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1096 - acc: 0.9924 - val_loss: 0.8777 - val_acc: 0.8843\n",
            "Epoch 162/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1123 - acc: 0.9918 - val_loss: 1.0211 - val_acc: 0.8817\n",
            "Epoch 163/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1149 - acc: 0.9916 - val_loss: 0.8437 - val_acc: 0.8887\n",
            "Epoch 164/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1077 - acc: 0.9921 - val_loss: 0.9409 - val_acc: 0.8883\n",
            "Epoch 165/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1032 - acc: 0.9935 - val_loss: 1.0192 - val_acc: 0.8893\n",
            "Epoch 166/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1168 - acc: 0.9920 - val_loss: 0.8636 - val_acc: 0.8927\n",
            "Epoch 167/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1097 - acc: 0.9921 - val_loss: 0.8214 - val_acc: 0.8870\n",
            "Epoch 168/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1141 - acc: 0.9916 - val_loss: 0.8582 - val_acc: 0.8863\n",
            "Epoch 169/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1096 - acc: 0.9922 - val_loss: 0.8499 - val_acc: 0.8843\n",
            "Epoch 170/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1068 - acc: 0.9933 - val_loss: 0.9105 - val_acc: 0.8893\n",
            "Epoch 171/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1110 - acc: 0.9921 - val_loss: 0.8580 - val_acc: 0.8863\n",
            "Epoch 172/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1082 - acc: 0.9922 - val_loss: 0.9765 - val_acc: 0.8860\n",
            "Epoch 173/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1066 - acc: 0.9928 - val_loss: 0.9049 - val_acc: 0.8863\n",
            "Epoch 174/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1066 - acc: 0.9934 - val_loss: 1.0205 - val_acc: 0.8860\n",
            "Epoch 175/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1059 - acc: 0.9921 - val_loss: 1.1497 - val_acc: 0.8790\n",
            "Epoch 176/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1017 - acc: 0.9928 - val_loss: 1.0466 - val_acc: 0.8790\n",
            "Epoch 177/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1139 - acc: 0.9910 - val_loss: 0.9409 - val_acc: 0.8873\n",
            "Epoch 178/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1078 - acc: 0.9926 - val_loss: 0.6695 - val_acc: 0.8907\n",
            "Epoch 179/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1034 - acc: 0.9931 - val_loss: 0.8809 - val_acc: 0.8817\n",
            "Epoch 180/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1049 - acc: 0.9928 - val_loss: 0.8972 - val_acc: 0.8910\n",
            "Epoch 181/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1094 - acc: 0.9921 - val_loss: 0.8495 - val_acc: 0.8927\n",
            "Epoch 182/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1102 - acc: 0.9921 - val_loss: 0.8392 - val_acc: 0.8927\n",
            "Epoch 183/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1069 - acc: 0.9923 - val_loss: 0.7409 - val_acc: 0.8870\n",
            "Epoch 184/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1061 - acc: 0.9926 - val_loss: 0.9897 - val_acc: 0.8827\n",
            "Epoch 185/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1087 - acc: 0.9927 - val_loss: 0.8674 - val_acc: 0.8930\n",
            "Epoch 186/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1072 - acc: 0.9926 - val_loss: 1.0975 - val_acc: 0.8693\n",
            "Epoch 187/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1080 - acc: 0.9921 - val_loss: 0.9085 - val_acc: 0.8950\n",
            "Epoch 188/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1114 - acc: 0.9916 - val_loss: 1.1237 - val_acc: 0.8633\n",
            "Epoch 189/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1049 - acc: 0.9923 - val_loss: 1.1004 - val_acc: 0.8877\n",
            "Epoch 190/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1067 - acc: 0.9926 - val_loss: 0.8267 - val_acc: 0.8953\n",
            "Epoch 191/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1072 - acc: 0.9930 - val_loss: 0.9671 - val_acc: 0.8850\n",
            "Epoch 192/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1127 - acc: 0.9919 - val_loss: 0.9222 - val_acc: 0.8757\n",
            "Epoch 193/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1064 - acc: 0.9911 - val_loss: 0.8284 - val_acc: 0.8983\n",
            "Epoch 194/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1063 - acc: 0.9926 - val_loss: 0.7742 - val_acc: 0.8967\n",
            "Epoch 195/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1009 - acc: 0.9922 - val_loss: 0.9014 - val_acc: 0.8927\n",
            "Epoch 196/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1037 - acc: 0.9941 - val_loss: 0.8996 - val_acc: 0.8840\n",
            "Epoch 197/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0993 - acc: 0.9929 - val_loss: 0.9789 - val_acc: 0.8943\n",
            "Epoch 198/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1012 - acc: 0.9933 - val_loss: 0.9361 - val_acc: 0.8877\n",
            "Epoch 199/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1051 - acc: 0.9933 - val_loss: 0.8411 - val_acc: 0.8957\n",
            "Epoch 200/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1099 - acc: 0.9916 - val_loss: 0.8104 - val_acc: 0.8960\n",
            "Epoch 201/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1007 - acc: 0.9926 - val_loss: 1.0839 - val_acc: 0.8837\n",
            "Epoch 202/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1077 - acc: 0.9911 - val_loss: 0.9236 - val_acc: 0.8773\n",
            "Epoch 203/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1011 - acc: 0.9936 - val_loss: 1.0192 - val_acc: 0.8863\n",
            "Epoch 204/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1082 - acc: 0.9921 - val_loss: 0.9417 - val_acc: 0.8887\n",
            "Epoch 205/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1071 - acc: 0.9920 - val_loss: 0.7582 - val_acc: 0.8937\n",
            "Epoch 206/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1014 - acc: 0.9942 - val_loss: 0.9680 - val_acc: 0.8823\n",
            "Epoch 207/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0982 - acc: 0.9936 - val_loss: 0.9080 - val_acc: 0.8913\n",
            "Epoch 208/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1008 - acc: 0.9931 - val_loss: 0.9005 - val_acc: 0.8887\n",
            "Epoch 209/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0997 - acc: 0.9935 - val_loss: 0.9441 - val_acc: 0.8873\n",
            "Epoch 210/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1116 - acc: 0.9915 - val_loss: 0.7770 - val_acc: 0.8920\n",
            "Epoch 211/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1016 - acc: 0.9934 - val_loss: 0.8927 - val_acc: 0.8850\n",
            "Epoch 212/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1033 - acc: 0.9919 - val_loss: 0.8855 - val_acc: 0.8827\n",
            "Epoch 213/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0981 - acc: 0.9937 - val_loss: 1.2034 - val_acc: 0.8823\n",
            "Epoch 214/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1017 - acc: 0.9925 - val_loss: 1.0896 - val_acc: 0.8857\n",
            "Epoch 215/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1027 - acc: 0.9925 - val_loss: 0.7921 - val_acc: 0.8883\n",
            "Epoch 216/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0988 - acc: 0.9928 - val_loss: 0.9435 - val_acc: 0.8853\n",
            "Epoch 217/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1022 - acc: 0.9923 - val_loss: 0.9466 - val_acc: 0.8913\n",
            "Epoch 218/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1021 - acc: 0.9945 - val_loss: 0.9883 - val_acc: 0.8897\n",
            "Epoch 219/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0974 - acc: 0.9948 - val_loss: 0.9429 - val_acc: 0.8887\n",
            "Epoch 220/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1023 - acc: 0.9936 - val_loss: 0.9379 - val_acc: 0.8917\n",
            "Epoch 221/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1028 - acc: 0.9936 - val_loss: 0.8002 - val_acc: 0.8883\n",
            "Epoch 222/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1052 - acc: 0.9927 - val_loss: 0.7651 - val_acc: 0.8907\n",
            "Epoch 223/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1011 - acc: 0.9932 - val_loss: 0.9836 - val_acc: 0.8880\n",
            "Epoch 224/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1067 - acc: 0.9926 - val_loss: 0.7720 - val_acc: 0.8930\n",
            "Epoch 225/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1034 - acc: 0.9939 - val_loss: 1.0287 - val_acc: 0.8940\n",
            "Epoch 226/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0966 - acc: 0.9934 - val_loss: 1.0975 - val_acc: 0.8780\n",
            "Epoch 227/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1013 - acc: 0.9932 - val_loss: 0.8994 - val_acc: 0.8807\n",
            "Epoch 228/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1008 - acc: 0.9930 - val_loss: 1.0835 - val_acc: 0.8820\n",
            "Epoch 229/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0965 - acc: 0.9931 - val_loss: 1.1003 - val_acc: 0.8680\n",
            "Epoch 230/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1034 - acc: 0.9930 - val_loss: 0.9413 - val_acc: 0.8840\n",
            "Epoch 231/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0988 - acc: 0.9926 - val_loss: 0.9065 - val_acc: 0.8873\n",
            "Epoch 232/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0971 - acc: 0.9940 - val_loss: 0.9041 - val_acc: 0.8813\n",
            "Epoch 233/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0954 - acc: 0.9947 - val_loss: 0.9742 - val_acc: 0.8900\n",
            "Epoch 234/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1052 - acc: 0.9930 - val_loss: 0.9635 - val_acc: 0.8877\n",
            "Epoch 235/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1012 - acc: 0.9932 - val_loss: 1.1521 - val_acc: 0.8880\n",
            "Epoch 236/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1057 - acc: 0.9916 - val_loss: 0.9855 - val_acc: 0.8950\n",
            "Epoch 237/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0964 - acc: 0.9931 - val_loss: 0.9137 - val_acc: 0.8897\n",
            "Epoch 238/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0942 - acc: 0.9933 - val_loss: 0.9871 - val_acc: 0.8893\n",
            "Epoch 239/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0976 - acc: 0.9942 - val_loss: 0.9808 - val_acc: 0.8890\n",
            "Epoch 240/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1006 - acc: 0.9932 - val_loss: 0.9037 - val_acc: 0.8840\n",
            "Epoch 241/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0937 - acc: 0.9941 - val_loss: 1.0545 - val_acc: 0.8927\n",
            "Epoch 242/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0985 - acc: 0.9936 - val_loss: 1.1230 - val_acc: 0.8830\n",
            "Epoch 243/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0993 - acc: 0.9945 - val_loss: 1.1487 - val_acc: 0.8803\n",
            "Epoch 244/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1019 - acc: 0.9928 - val_loss: 0.9548 - val_acc: 0.8813\n",
            "Epoch 245/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0965 - acc: 0.9933 - val_loss: 0.9934 - val_acc: 0.8880\n",
            "Epoch 246/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0978 - acc: 0.9934 - val_loss: 0.7913 - val_acc: 0.8797\n",
            "Epoch 247/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0950 - acc: 0.9946 - val_loss: 0.9605 - val_acc: 0.8830\n",
            "Epoch 248/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1002 - acc: 0.9932 - val_loss: 1.0753 - val_acc: 0.8777\n",
            "Epoch 249/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1030 - acc: 0.9936 - val_loss: 0.9280 - val_acc: 0.8860\n",
            "Epoch 250/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0994 - acc: 0.9945 - val_loss: 0.9350 - val_acc: 0.8890\n",
            "Epoch 251/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0999 - acc: 0.9931 - val_loss: 1.2541 - val_acc: 0.8657\n",
            "Epoch 252/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1024 - acc: 0.9941 - val_loss: 1.0198 - val_acc: 0.8883\n",
            "Epoch 253/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.1062 - acc: 0.9931 - val_loss: 0.9634 - val_acc: 0.8877\n",
            "Epoch 254/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0973 - acc: 0.9936 - val_loss: 0.9179 - val_acc: 0.8870\n",
            "Epoch 255/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0936 - acc: 0.9936 - val_loss: 0.9634 - val_acc: 0.8843\n",
            "Epoch 256/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0987 - acc: 0.9933 - val_loss: 0.9307 - val_acc: 0.8883\n",
            "Epoch 257/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1012 - acc: 0.9918 - val_loss: 1.0636 - val_acc: 0.8830\n",
            "Epoch 258/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0982 - acc: 0.9926 - val_loss: 0.9902 - val_acc: 0.8897\n",
            "Epoch 259/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1020 - acc: 0.9928 - val_loss: 0.9614 - val_acc: 0.8830\n",
            "Epoch 260/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0965 - acc: 0.9946 - val_loss: 0.9903 - val_acc: 0.8883\n",
            "Epoch 261/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0951 - acc: 0.9938 - val_loss: 0.8360 - val_acc: 0.8850\n",
            "Epoch 262/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0990 - acc: 0.9945 - val_loss: 0.9072 - val_acc: 0.8847\n",
            "Epoch 263/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0990 - acc: 0.9928 - val_loss: 0.9341 - val_acc: 0.8920\n",
            "Epoch 264/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0962 - acc: 0.9940 - val_loss: 1.0108 - val_acc: 0.8903\n",
            "Epoch 265/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0930 - acc: 0.9947 - val_loss: 1.0311 - val_acc: 0.8883\n",
            "Epoch 266/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0952 - acc: 0.9948 - val_loss: 1.0506 - val_acc: 0.8917\n",
            "Epoch 267/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0914 - acc: 0.9942 - val_loss: 1.1188 - val_acc: 0.8933\n",
            "Epoch 268/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0931 - acc: 0.9938 - val_loss: 0.9589 - val_acc: 0.8893\n",
            "Epoch 269/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0933 - acc: 0.9945 - val_loss: 0.8802 - val_acc: 0.8873\n",
            "Epoch 270/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0927 - acc: 0.9943 - val_loss: 0.8771 - val_acc: 0.8847\n",
            "Epoch 271/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0978 - acc: 0.9936 - val_loss: 0.8657 - val_acc: 0.8853\n",
            "Epoch 272/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0900 - acc: 0.9945 - val_loss: 0.9107 - val_acc: 0.8897\n",
            "Epoch 273/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0932 - acc: 0.9945 - val_loss: 1.0629 - val_acc: 0.8837\n",
            "Epoch 274/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0919 - acc: 0.9935 - val_loss: 0.9733 - val_acc: 0.8903\n",
            "Epoch 275/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0986 - acc: 0.9942 - val_loss: 1.0441 - val_acc: 0.8927\n",
            "Epoch 276/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1040 - acc: 0.9931 - val_loss: 0.9410 - val_acc: 0.8870\n",
            "Epoch 277/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1034 - acc: 0.9928 - val_loss: 0.8559 - val_acc: 0.8970\n",
            "Epoch 278/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0931 - acc: 0.9938 - val_loss: 0.9650 - val_acc: 0.8847\n",
            "Epoch 279/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0946 - acc: 0.9946 - val_loss: 0.9930 - val_acc: 0.8900\n",
            "Epoch 280/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0885 - acc: 0.9942 - val_loss: 0.8956 - val_acc: 0.8910\n",
            "Epoch 281/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0950 - acc: 0.9932 - val_loss: 0.8269 - val_acc: 0.8737\n",
            "Epoch 282/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0882 - acc: 0.9942 - val_loss: 0.9070 - val_acc: 0.8887\n",
            "Epoch 283/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0975 - acc: 0.9942 - val_loss: 0.8716 - val_acc: 0.8940\n",
            "Epoch 284/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0936 - acc: 0.9943 - val_loss: 0.8995 - val_acc: 0.8957\n",
            "Epoch 285/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0915 - acc: 0.9941 - val_loss: 0.9381 - val_acc: 0.8870\n",
            "Epoch 286/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0922 - acc: 0.9931 - val_loss: 0.9889 - val_acc: 0.8820\n",
            "Epoch 287/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0949 - acc: 0.9945 - val_loss: 0.9784 - val_acc: 0.8857\n",
            "Epoch 288/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1022 - acc: 0.9936 - val_loss: 1.0443 - val_acc: 0.8893\n",
            "Epoch 289/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0914 - acc: 0.9930 - val_loss: 0.8504 - val_acc: 0.8900\n",
            "Epoch 290/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0898 - acc: 0.9947 - val_loss: 1.1667 - val_acc: 0.8730\n",
            "Epoch 291/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0969 - acc: 0.9939 - val_loss: 0.9702 - val_acc: 0.8960\n",
            "Epoch 292/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.1023 - acc: 0.9930 - val_loss: 0.9875 - val_acc: 0.8787\n",
            "Epoch 293/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0936 - acc: 0.9928 - val_loss: 0.8074 - val_acc: 0.8900\n",
            "Epoch 294/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0869 - acc: 0.9942 - val_loss: 0.9606 - val_acc: 0.8890\n",
            "Epoch 295/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0890 - acc: 0.9940 - val_loss: 1.0283 - val_acc: 0.8820\n",
            "Epoch 296/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.1004 - acc: 0.9933 - val_loss: 0.9515 - val_acc: 0.8900\n",
            "Epoch 297/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0964 - acc: 0.9946 - val_loss: 0.8920 - val_acc: 0.8880\n",
            "Epoch 298/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0985 - acc: 0.9931 - val_loss: 0.8944 - val_acc: 0.8853\n",
            "Epoch 299/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0916 - acc: 0.9936 - val_loss: 0.8323 - val_acc: 0.8850\n",
            "Epoch 300/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0947 - acc: 0.9931 - val_loss: 0.9167 - val_acc: 0.8907\n",
            "Epoch 301/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0915 - acc: 0.9939 - val_loss: 0.9214 - val_acc: 0.8820\n",
            "Epoch 302/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0839 - acc: 0.9959 - val_loss: 0.9330 - val_acc: 0.8870\n",
            "Epoch 303/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0779 - acc: 0.9963 - val_loss: 0.9379 - val_acc: 0.8853\n",
            "Epoch 304/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0798 - acc: 0.9967 - val_loss: 0.9406 - val_acc: 0.8883\n",
            "Epoch 305/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0867 - acc: 0.9960 - val_loss: 0.9273 - val_acc: 0.8873\n",
            "Epoch 306/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0790 - acc: 0.9964 - val_loss: 1.0357 - val_acc: 0.8903\n",
            "Epoch 307/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0773 - acc: 0.9967 - val_loss: 0.9486 - val_acc: 0.8927\n",
            "Epoch 308/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0837 - acc: 0.9967 - val_loss: 0.9913 - val_acc: 0.8903\n",
            "Epoch 309/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0788 - acc: 0.9967 - val_loss: 1.0284 - val_acc: 0.8890\n",
            "Epoch 310/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0821 - acc: 0.9971 - val_loss: 0.9741 - val_acc: 0.8940\n",
            "Epoch 311/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0763 - acc: 0.9968 - val_loss: 0.9854 - val_acc: 0.8943\n",
            "Epoch 312/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0819 - acc: 0.9967 - val_loss: 1.0448 - val_acc: 0.8917\n",
            "Epoch 313/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0783 - acc: 0.9979 - val_loss: 1.0153 - val_acc: 0.8930\n",
            "Epoch 314/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0802 - acc: 0.9969 - val_loss: 1.0502 - val_acc: 0.8937\n",
            "Epoch 315/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0813 - acc: 0.9969 - val_loss: 1.0130 - val_acc: 0.8963\n",
            "Epoch 316/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0772 - acc: 0.9970 - val_loss: 1.0744 - val_acc: 0.8990\n",
            "Epoch 317/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0782 - acc: 0.9972 - val_loss: 1.0068 - val_acc: 0.8960\n",
            "Epoch 318/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0806 - acc: 0.9969 - val_loss: 0.9958 - val_acc: 0.8940\n",
            "Epoch 319/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0757 - acc: 0.9970 - val_loss: 0.9787 - val_acc: 0.8943\n",
            "Epoch 320/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0797 - acc: 0.9968 - val_loss: 0.9722 - val_acc: 0.8993\n",
            "Epoch 321/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0809 - acc: 0.9974 - val_loss: 1.0819 - val_acc: 0.8940\n",
            "Epoch 322/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0775 - acc: 0.9966 - val_loss: 1.0536 - val_acc: 0.8933\n",
            "Epoch 323/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0776 - acc: 0.9977 - val_loss: 1.0937 - val_acc: 0.8947\n",
            "Epoch 324/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0869 - acc: 0.9966 - val_loss: 1.0915 - val_acc: 0.8940\n",
            "Epoch 325/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0820 - acc: 0.9973 - val_loss: 1.1343 - val_acc: 0.8933\n",
            "Epoch 326/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0752 - acc: 0.9976 - val_loss: 1.1175 - val_acc: 0.8893\n",
            "Epoch 327/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0788 - acc: 0.9971 - val_loss: 1.1401 - val_acc: 0.8940\n",
            "Epoch 328/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0739 - acc: 0.9977 - val_loss: 1.1175 - val_acc: 0.8910\n",
            "Epoch 329/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0770 - acc: 0.9977 - val_loss: 1.1183 - val_acc: 0.8930\n",
            "Epoch 330/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0824 - acc: 0.9967 - val_loss: 1.0853 - val_acc: 0.8937\n",
            "Epoch 331/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0753 - acc: 0.9973 - val_loss: 1.0383 - val_acc: 0.8940\n",
            "Epoch 332/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0875 - acc: 0.9967 - val_loss: 1.0440 - val_acc: 0.8970\n",
            "Epoch 333/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0764 - acc: 0.9971 - val_loss: 0.9904 - val_acc: 0.8920\n",
            "Epoch 334/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0778 - acc: 0.9977 - val_loss: 0.9973 - val_acc: 0.8913\n",
            "Epoch 335/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0785 - acc: 0.9968 - val_loss: 0.9318 - val_acc: 0.8960\n",
            "Epoch 336/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0752 - acc: 0.9969 - val_loss: 1.0421 - val_acc: 0.8943\n",
            "Epoch 337/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0817 - acc: 0.9968 - val_loss: 1.0109 - val_acc: 0.8930\n",
            "Epoch 338/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0854 - acc: 0.9964 - val_loss: 0.9553 - val_acc: 0.8920\n",
            "Epoch 339/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0750 - acc: 0.9979 - val_loss: 1.0299 - val_acc: 0.8940\n",
            "Epoch 340/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0747 - acc: 0.9978 - val_loss: 0.9610 - val_acc: 0.8977\n",
            "Epoch 341/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0758 - acc: 0.9977 - val_loss: 1.1398 - val_acc: 0.8930\n",
            "Epoch 342/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0763 - acc: 0.9973 - val_loss: 1.1048 - val_acc: 0.8940\n",
            "Epoch 343/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0709 - acc: 0.9979 - val_loss: 1.0997 - val_acc: 0.8950\n",
            "Epoch 344/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0788 - acc: 0.9967 - val_loss: 1.0372 - val_acc: 0.8907\n",
            "Epoch 345/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0729 - acc: 0.9978 - val_loss: 1.2064 - val_acc: 0.8923\n",
            "Epoch 346/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0728 - acc: 0.9980 - val_loss: 1.1007 - val_acc: 0.8953\n",
            "Epoch 347/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0760 - acc: 0.9973 - val_loss: 1.0506 - val_acc: 0.8933\n",
            "Epoch 348/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0794 - acc: 0.9968 - val_loss: 0.9955 - val_acc: 0.8960\n",
            "Epoch 349/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0745 - acc: 0.9979 - val_loss: 1.1893 - val_acc: 0.8973\n",
            "Epoch 350/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0789 - acc: 0.9977 - val_loss: 1.0740 - val_acc: 0.8920\n",
            "Epoch 351/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0776 - acc: 0.9972 - val_loss: 0.9715 - val_acc: 0.8953\n",
            "Epoch 352/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0803 - acc: 0.9964 - val_loss: 0.9327 - val_acc: 0.8937\n",
            "Epoch 353/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0724 - acc: 0.9975 - val_loss: 1.0428 - val_acc: 0.8960\n",
            "Epoch 354/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0785 - acc: 0.9970 - val_loss: 1.0466 - val_acc: 0.8920\n",
            "Epoch 355/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0725 - acc: 0.9975 - val_loss: 0.9948 - val_acc: 0.8910\n",
            "Epoch 356/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0741 - acc: 0.9977 - val_loss: 1.0563 - val_acc: 0.8903\n",
            "Epoch 357/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0722 - acc: 0.9981 - val_loss: 0.9793 - val_acc: 0.8910\n",
            "Epoch 358/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0774 - acc: 0.9975 - val_loss: 1.1153 - val_acc: 0.8950\n",
            "Epoch 359/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0735 - acc: 0.9970 - val_loss: 1.0615 - val_acc: 0.8967\n",
            "Epoch 360/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0726 - acc: 0.9977 - val_loss: 1.0275 - val_acc: 0.8980\n",
            "Epoch 361/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0782 - acc: 0.9974 - val_loss: 1.0862 - val_acc: 0.8933\n",
            "Epoch 362/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0817 - acc: 0.9973 - val_loss: 1.0555 - val_acc: 0.8967\n",
            "Epoch 363/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0785 - acc: 0.9972 - val_loss: 0.9874 - val_acc: 0.8923\n",
            "Epoch 364/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0755 - acc: 0.9977 - val_loss: 0.9468 - val_acc: 0.8937\n",
            "Epoch 365/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0736 - acc: 0.9978 - val_loss: 0.9757 - val_acc: 0.8953\n",
            "Epoch 366/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0745 - acc: 0.9974 - val_loss: 1.0603 - val_acc: 0.8927\n",
            "Epoch 367/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0766 - acc: 0.9979 - val_loss: 1.0230 - val_acc: 0.8950\n",
            "Epoch 368/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0739 - acc: 0.9978 - val_loss: 1.2026 - val_acc: 0.8953\n",
            "Epoch 369/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0783 - acc: 0.9974 - val_loss: 1.0037 - val_acc: 0.8960\n",
            "Epoch 370/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0737 - acc: 0.9974 - val_loss: 1.0913 - val_acc: 0.8910\n",
            "Epoch 371/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0737 - acc: 0.9972 - val_loss: 0.9474 - val_acc: 0.8953\n",
            "Epoch 372/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0763 - acc: 0.9977 - val_loss: 0.9989 - val_acc: 0.8923\n",
            "Epoch 373/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0725 - acc: 0.9973 - val_loss: 0.9833 - val_acc: 0.8973\n",
            "Epoch 374/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0717 - acc: 0.9972 - val_loss: 1.0087 - val_acc: 0.8950\n",
            "Epoch 375/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0779 - acc: 0.9968 - val_loss: 1.0147 - val_acc: 0.8963\n",
            "Epoch 376/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0780 - acc: 0.9976 - val_loss: 1.1921 - val_acc: 0.8963\n",
            "Epoch 377/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0674 - acc: 0.9987 - val_loss: 1.3019 - val_acc: 0.8950\n",
            "Epoch 378/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0744 - acc: 0.9974 - val_loss: 1.1029 - val_acc: 0.8947\n",
            "Epoch 379/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0715 - acc: 0.9976 - val_loss: 0.9676 - val_acc: 0.8917\n",
            "Epoch 380/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0742 - acc: 0.9978 - val_loss: 1.0534 - val_acc: 0.8940\n",
            "Epoch 381/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0688 - acc: 0.9974 - val_loss: 0.9774 - val_acc: 0.8867\n",
            "Epoch 382/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0668 - acc: 0.9980 - val_loss: 1.0413 - val_acc: 0.8943\n",
            "Epoch 383/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0712 - acc: 0.9982 - val_loss: 1.2470 - val_acc: 0.8940\n",
            "Epoch 384/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0698 - acc: 0.9980 - val_loss: 1.1692 - val_acc: 0.8920\n",
            "Epoch 385/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0771 - acc: 0.9976 - val_loss: 1.2117 - val_acc: 0.8950\n",
            "Epoch 386/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0710 - acc: 0.9980 - val_loss: 1.0317 - val_acc: 0.8943\n",
            "Epoch 387/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0778 - acc: 0.9974 - val_loss: 0.9683 - val_acc: 0.8947\n",
            "Epoch 388/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0698 - acc: 0.9975 - val_loss: 1.0870 - val_acc: 0.8943\n",
            "Epoch 389/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0723 - acc: 0.9981 - val_loss: 1.1464 - val_acc: 0.8927\n",
            "Epoch 390/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0737 - acc: 0.9978 - val_loss: 1.1795 - val_acc: 0.8887\n",
            "Epoch 391/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0728 - acc: 0.9982 - val_loss: 1.3001 - val_acc: 0.8923\n",
            "Epoch 392/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0729 - acc: 0.9972 - val_loss: 1.1564 - val_acc: 0.8930\n",
            "Epoch 393/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0777 - acc: 0.9969 - val_loss: 1.0622 - val_acc: 0.8900\n",
            "Epoch 394/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0699 - acc: 0.9977 - val_loss: 1.0750 - val_acc: 0.8970\n",
            "Epoch 395/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0722 - acc: 0.9973 - val_loss: 1.0676 - val_acc: 0.8970\n",
            "Epoch 396/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0710 - acc: 0.9977 - val_loss: 1.1190 - val_acc: 0.8903\n",
            "Epoch 397/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0714 - acc: 0.9974 - val_loss: 1.0318 - val_acc: 0.8957\n",
            "Epoch 398/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0705 - acc: 0.9973 - val_loss: 1.0138 - val_acc: 0.8957\n",
            "Epoch 399/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0689 - acc: 0.9974 - val_loss: 1.2142 - val_acc: 0.8877\n",
            "Epoch 400/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0765 - acc: 0.9967 - val_loss: 0.9756 - val_acc: 0.8957\n",
            "Epoch 401/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0725 - acc: 0.9976 - val_loss: 1.0952 - val_acc: 0.8907\n",
            "Epoch 402/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0701 - acc: 0.9980 - val_loss: 1.1268 - val_acc: 0.8933\n",
            "Epoch 403/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0698 - acc: 0.9973 - val_loss: 1.1103 - val_acc: 0.8973\n",
            "Epoch 404/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0792 - acc: 0.9965 - val_loss: 1.0470 - val_acc: 0.8953\n",
            "Epoch 405/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0739 - acc: 0.9978 - val_loss: 1.0363 - val_acc: 0.8987\n",
            "Epoch 406/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0703 - acc: 0.9980 - val_loss: 1.1166 - val_acc: 0.8973\n",
            "Epoch 407/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0700 - acc: 0.9973 - val_loss: 1.1074 - val_acc: 0.8967\n",
            "Epoch 408/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0727 - acc: 0.9977 - val_loss: 1.1138 - val_acc: 0.8873\n",
            "Epoch 409/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0762 - acc: 0.9969 - val_loss: 1.0298 - val_acc: 0.8927\n",
            "Epoch 410/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0738 - acc: 0.9977 - val_loss: 1.3010 - val_acc: 0.8957\n",
            "Epoch 411/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0724 - acc: 0.9976 - val_loss: 1.1355 - val_acc: 0.8950\n",
            "Epoch 412/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0705 - acc: 0.9977 - val_loss: 1.1562 - val_acc: 0.8927\n",
            "Epoch 413/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0663 - acc: 0.9989 - val_loss: 1.3105 - val_acc: 0.8993\n",
            "Epoch 414/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0727 - acc: 0.9978 - val_loss: 1.2750 - val_acc: 0.8923\n",
            "Epoch 415/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0704 - acc: 0.9984 - val_loss: 1.2541 - val_acc: 0.8950\n",
            "Epoch 416/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0711 - acc: 0.9979 - val_loss: 1.1738 - val_acc: 0.8953\n",
            "Epoch 417/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0682 - acc: 0.9980 - val_loss: 1.1325 - val_acc: 0.8953\n",
            "Epoch 418/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0645 - acc: 0.9985 - val_loss: 1.2359 - val_acc: 0.8977\n",
            "Epoch 419/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0713 - acc: 0.9979 - val_loss: 1.1149 - val_acc: 0.8973\n",
            "Epoch 420/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0709 - acc: 0.9976 - val_loss: 1.0790 - val_acc: 0.8990\n",
            "Epoch 421/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0692 - acc: 0.9975 - val_loss: 0.9976 - val_acc: 0.9000\n",
            "Epoch 422/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0685 - acc: 0.9980 - val_loss: 1.0526 - val_acc: 0.8930\n",
            "Epoch 423/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0661 - acc: 0.9978 - val_loss: 1.1171 - val_acc: 0.8987\n",
            "Epoch 424/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0691 - acc: 0.9981 - val_loss: 1.0522 - val_acc: 0.8950\n",
            "Epoch 425/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0688 - acc: 0.9980 - val_loss: 1.0814 - val_acc: 0.8967\n",
            "Epoch 426/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0675 - acc: 0.9978 - val_loss: 1.0416 - val_acc: 0.8953\n",
            "Epoch 427/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0678 - acc: 0.9977 - val_loss: 1.1379 - val_acc: 0.8933\n",
            "Epoch 428/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0728 - acc: 0.9980 - val_loss: 1.2150 - val_acc: 0.8910\n",
            "Epoch 429/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0705 - acc: 0.9980 - val_loss: 1.0773 - val_acc: 0.8937\n",
            "Epoch 430/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0707 - acc: 0.9974 - val_loss: 1.0399 - val_acc: 0.8973\n",
            "Epoch 431/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0729 - acc: 0.9967 - val_loss: 0.9435 - val_acc: 0.8903\n",
            "Epoch 432/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0725 - acc: 0.9968 - val_loss: 0.9852 - val_acc: 0.8920\n",
            "Epoch 433/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0657 - acc: 0.9984 - val_loss: 1.1672 - val_acc: 0.8970\n",
            "Epoch 434/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0675 - acc: 0.9972 - val_loss: 1.0403 - val_acc: 0.8937\n",
            "Epoch 435/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0669 - acc: 0.9976 - val_loss: 1.0271 - val_acc: 0.8927\n",
            "Epoch 436/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0670 - acc: 0.9983 - val_loss: 1.1838 - val_acc: 0.8927\n",
            "Epoch 437/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0698 - acc: 0.9974 - val_loss: 0.9883 - val_acc: 0.8963\n",
            "Epoch 438/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0668 - acc: 0.9970 - val_loss: 0.9967 - val_acc: 0.8960\n",
            "Epoch 439/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0656 - acc: 0.9986 - val_loss: 1.1014 - val_acc: 0.8903\n",
            "Epoch 440/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0672 - acc: 0.9978 - val_loss: 1.0424 - val_acc: 0.8947\n",
            "Epoch 441/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0640 - acc: 0.9986 - val_loss: 1.1248 - val_acc: 0.8953\n",
            "Epoch 442/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0657 - acc: 0.9987 - val_loss: 1.1549 - val_acc: 0.8967\n",
            "Epoch 443/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0712 - acc: 0.9974 - val_loss: 1.1201 - val_acc: 0.8953\n",
            "Epoch 444/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0667 - acc: 0.9981 - val_loss: 1.1481 - val_acc: 0.8937\n",
            "Epoch 445/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0632 - acc: 0.9980 - val_loss: 1.0582 - val_acc: 0.8957\n",
            "Epoch 446/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0689 - acc: 0.9975 - val_loss: 1.0661 - val_acc: 0.8970\n",
            "Epoch 447/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0675 - acc: 0.9981 - val_loss: 1.1125 - val_acc: 0.8897\n",
            "Epoch 448/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0650 - acc: 0.9983 - val_loss: 1.1836 - val_acc: 0.8933\n",
            "Epoch 449/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0678 - acc: 0.9975 - val_loss: 0.9282 - val_acc: 0.8943\n",
            "Epoch 450/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0642 - acc: 0.9977 - val_loss: 1.0324 - val_acc: 0.8977\n",
            "Epoch 451/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0695 - acc: 0.9973 - val_loss: 1.0319 - val_acc: 0.8940\n",
            "Epoch 452/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0630 - acc: 0.9981 - val_loss: 1.0726 - val_acc: 0.8933\n",
            "Epoch 453/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0671 - acc: 0.9978 - val_loss: 1.1300 - val_acc: 0.8877\n",
            "Epoch 454/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0654 - acc: 0.9982 - val_loss: 1.1715 - val_acc: 0.8920\n",
            "Epoch 455/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0677 - acc: 0.9977 - val_loss: 1.0536 - val_acc: 0.8883\n",
            "Epoch 456/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0625 - acc: 0.9981 - val_loss: 1.1690 - val_acc: 0.8920\n",
            "Epoch 457/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0646 - acc: 0.9985 - val_loss: 1.2334 - val_acc: 0.8947\n",
            "Epoch 458/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0677 - acc: 0.9984 - val_loss: 1.2264 - val_acc: 0.8910\n",
            "Epoch 459/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0639 - acc: 0.9982 - val_loss: 1.1683 - val_acc: 0.8920\n",
            "Epoch 460/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0643 - acc: 0.9984 - val_loss: 1.2797 - val_acc: 0.8893\n",
            "Epoch 461/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0682 - acc: 0.9976 - val_loss: 1.3182 - val_acc: 0.8870\n",
            "Epoch 462/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0668 - acc: 0.9980 - val_loss: 1.1320 - val_acc: 0.8940\n",
            "Epoch 463/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0682 - acc: 0.9982 - val_loss: 1.1981 - val_acc: 0.8957\n",
            "Epoch 464/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0690 - acc: 0.9973 - val_loss: 1.1152 - val_acc: 0.8933\n",
            "Epoch 465/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0689 - acc: 0.9975 - val_loss: 1.2103 - val_acc: 0.8923\n",
            "Epoch 466/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0666 - acc: 0.9979 - val_loss: 1.3084 - val_acc: 0.8900\n",
            "Epoch 467/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0639 - acc: 0.9982 - val_loss: 1.3093 - val_acc: 0.8947\n",
            "Epoch 468/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0658 - acc: 0.9985 - val_loss: 1.4815 - val_acc: 0.8917\n",
            "Epoch 469/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0684 - acc: 0.9977 - val_loss: 1.3422 - val_acc: 0.8883\n",
            "Epoch 470/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0623 - acc: 0.9982 - val_loss: 1.2774 - val_acc: 0.8903\n",
            "Epoch 471/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0666 - acc: 0.9982 - val_loss: 1.1163 - val_acc: 0.8943\n",
            "Epoch 472/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0699 - acc: 0.9972 - val_loss: 1.0897 - val_acc: 0.8950\n",
            "Epoch 473/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0701 - acc: 0.9976 - val_loss: 1.1844 - val_acc: 0.8897\n",
            "Epoch 474/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0686 - acc: 0.9977 - val_loss: 1.2513 - val_acc: 0.8857\n",
            "Epoch 475/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0654 - acc: 0.9975 - val_loss: 1.0674 - val_acc: 0.8903\n",
            "Epoch 476/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0654 - acc: 0.9979 - val_loss: 1.1313 - val_acc: 0.8927\n",
            "Epoch 477/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0647 - acc: 0.9982 - val_loss: 1.1036 - val_acc: 0.8897\n",
            "Epoch 478/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0626 - acc: 0.9987 - val_loss: 1.3440 - val_acc: 0.8927\n",
            "Epoch 479/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0681 - acc: 0.9970 - val_loss: 1.1900 - val_acc: 0.8903\n",
            "Epoch 480/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0627 - acc: 0.9980 - val_loss: 1.0282 - val_acc: 0.8930\n",
            "Epoch 481/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0683 - acc: 0.9972 - val_loss: 1.0587 - val_acc: 0.8903\n",
            "Epoch 482/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0662 - acc: 0.9978 - val_loss: 1.1943 - val_acc: 0.8920\n",
            "Epoch 483/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0686 - acc: 0.9972 - val_loss: 1.0082 - val_acc: 0.8933\n",
            "Epoch 484/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0638 - acc: 0.9987 - val_loss: 1.2722 - val_acc: 0.8883\n",
            "Epoch 485/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0712 - acc: 0.9969 - val_loss: 0.9684 - val_acc: 0.8930\n",
            "Epoch 486/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0633 - acc: 0.9981 - val_loss: 1.1004 - val_acc: 0.8927\n",
            "Epoch 487/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0672 - acc: 0.9972 - val_loss: 1.1996 - val_acc: 0.8890\n",
            "Epoch 488/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0657 - acc: 0.9982 - val_loss: 1.1575 - val_acc: 0.8903\n",
            "Epoch 489/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0649 - acc: 0.9982 - val_loss: 1.1452 - val_acc: 0.8957\n",
            "Epoch 490/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0665 - acc: 0.9973 - val_loss: 1.1074 - val_acc: 0.8967\n",
            "Epoch 491/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0627 - acc: 0.9976 - val_loss: 1.1713 - val_acc: 0.8907\n",
            "Epoch 492/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0625 - acc: 0.9984 - val_loss: 1.2587 - val_acc: 0.8917\n",
            "Epoch 493/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0653 - acc: 0.9974 - val_loss: 1.1048 - val_acc: 0.8927\n",
            "Epoch 494/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0641 - acc: 0.9977 - val_loss: 1.1440 - val_acc: 0.8900\n",
            "Epoch 495/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0639 - acc: 0.9978 - val_loss: 1.0718 - val_acc: 0.8923\n",
            "Epoch 496/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0627 - acc: 0.9977 - val_loss: 1.1887 - val_acc: 0.8930\n",
            "Epoch 497/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0651 - acc: 0.9973 - val_loss: 1.0382 - val_acc: 0.8907\n",
            "Epoch 498/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0630 - acc: 0.9978 - val_loss: 1.0886 - val_acc: 0.8907\n",
            "Epoch 499/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0670 - acc: 0.9977 - val_loss: 1.1277 - val_acc: 0.8933\n",
            "Epoch 500/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0697 - acc: 0.9977 - val_loss: 1.1865 - val_acc: 0.8957\n",
            "Epoch 501/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0681 - acc: 0.9977 - val_loss: 1.1453 - val_acc: 0.8937\n",
            "Epoch 502/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0646 - acc: 0.9980 - val_loss: 1.1659 - val_acc: 0.8943\n",
            "Epoch 503/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0694 - acc: 0.9974 - val_loss: 1.1298 - val_acc: 0.8913\n",
            "Epoch 504/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0678 - acc: 0.9975 - val_loss: 1.1445 - val_acc: 0.8937\n",
            "Epoch 505/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0631 - acc: 0.9977 - val_loss: 1.1807 - val_acc: 0.8933\n",
            "Epoch 506/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0645 - acc: 0.9982 - val_loss: 1.2640 - val_acc: 0.8907\n",
            "Epoch 507/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0675 - acc: 0.9968 - val_loss: 1.0607 - val_acc: 0.8907\n",
            "Epoch 508/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0704 - acc: 0.9977 - val_loss: 1.2367 - val_acc: 0.8913\n",
            "Epoch 509/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0734 - acc: 0.9971 - val_loss: 0.9715 - val_acc: 0.8927\n",
            "Epoch 510/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0628 - acc: 0.9983 - val_loss: 1.1371 - val_acc: 0.8960\n",
            "Epoch 511/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0615 - acc: 0.9982 - val_loss: 1.0964 - val_acc: 0.8927\n",
            "Epoch 512/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0630 - acc: 0.9983 - val_loss: 1.1278 - val_acc: 0.8940\n",
            "Epoch 513/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0652 - acc: 0.9980 - val_loss: 1.1546 - val_acc: 0.8893\n",
            "Epoch 514/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0670 - acc: 0.9977 - val_loss: 1.1696 - val_acc: 0.8860\n",
            "Epoch 515/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0670 - acc: 0.9980 - val_loss: 1.2148 - val_acc: 0.8893\n",
            "Epoch 516/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0595 - acc: 0.9982 - val_loss: 1.0879 - val_acc: 0.8880\n",
            "Epoch 517/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0657 - acc: 0.9976 - val_loss: 1.2169 - val_acc: 0.8880\n",
            "Epoch 518/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0654 - acc: 0.9975 - val_loss: 1.1608 - val_acc: 0.8870\n",
            "Epoch 519/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0636 - acc: 0.9977 - val_loss: 1.1429 - val_acc: 0.8863\n",
            "Epoch 520/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0635 - acc: 0.9972 - val_loss: 0.9352 - val_acc: 0.8890\n",
            "Epoch 521/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0659 - acc: 0.9974 - val_loss: 1.1411 - val_acc: 0.8873\n",
            "Epoch 522/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0657 - acc: 0.9977 - val_loss: 1.0928 - val_acc: 0.8917\n",
            "Epoch 523/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0612 - acc: 0.9976 - val_loss: 1.1205 - val_acc: 0.8917\n",
            "Epoch 524/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0625 - acc: 0.9977 - val_loss: 1.0823 - val_acc: 0.8957\n",
            "Epoch 525/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0654 - acc: 0.9973 - val_loss: 1.0008 - val_acc: 0.8917\n",
            "Epoch 526/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0626 - acc: 0.9977 - val_loss: 1.1681 - val_acc: 0.8860\n",
            "Epoch 527/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0625 - acc: 0.9974 - val_loss: 1.2015 - val_acc: 0.8910\n",
            "Epoch 528/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0676 - acc: 0.9976 - val_loss: 1.0100 - val_acc: 0.8970\n",
            "Epoch 529/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0643 - acc: 0.9984 - val_loss: 1.0156 - val_acc: 0.8947\n",
            "Epoch 530/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0632 - acc: 0.9987 - val_loss: 1.2347 - val_acc: 0.8923\n",
            "Epoch 531/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0692 - acc: 0.9974 - val_loss: 1.0260 - val_acc: 0.8937\n",
            "Epoch 532/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0630 - acc: 0.9973 - val_loss: 0.9810 - val_acc: 0.8943\n",
            "Epoch 533/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0636 - acc: 0.9977 - val_loss: 1.0265 - val_acc: 0.8977\n",
            "Epoch 534/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0560 - acc: 0.9988 - val_loss: 1.1948 - val_acc: 0.8940\n",
            "Epoch 535/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0625 - acc: 0.9979 - val_loss: 1.2588 - val_acc: 0.8900\n",
            "Epoch 536/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0650 - acc: 0.9982 - val_loss: 1.1833 - val_acc: 0.8927\n",
            "Epoch 537/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0600 - acc: 0.9988 - val_loss: 1.2468 - val_acc: 0.8937\n",
            "Epoch 538/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0662 - acc: 0.9980 - val_loss: 1.2044 - val_acc: 0.8960\n",
            "Epoch 539/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0655 - acc: 0.9976 - val_loss: 1.2365 - val_acc: 0.8973\n",
            "Epoch 540/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0612 - acc: 0.9982 - val_loss: 1.0543 - val_acc: 0.8950\n",
            "Epoch 541/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0641 - acc: 0.9974 - val_loss: 1.1549 - val_acc: 0.8907\n",
            "Epoch 542/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0672 - acc: 0.9977 - val_loss: 1.1890 - val_acc: 0.8920\n",
            "Epoch 543/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0617 - acc: 0.9982 - val_loss: 1.2504 - val_acc: 0.8927\n",
            "Epoch 544/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0656 - acc: 0.9974 - val_loss: 1.1824 - val_acc: 0.8897\n",
            "Epoch 545/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0642 - acc: 0.9972 - val_loss: 1.0747 - val_acc: 0.8897\n",
            "Epoch 546/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0592 - acc: 0.9984 - val_loss: 1.1669 - val_acc: 0.8893\n",
            "Epoch 547/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0635 - acc: 0.9972 - val_loss: 1.1534 - val_acc: 0.8947\n",
            "Epoch 548/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0621 - acc: 0.9984 - val_loss: 1.2233 - val_acc: 0.8900\n",
            "Epoch 549/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0688 - acc: 0.9981 - val_loss: 1.2919 - val_acc: 0.8863\n",
            "Epoch 550/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0657 - acc: 0.9977 - val_loss: 1.1617 - val_acc: 0.8920\n",
            "Epoch 551/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0641 - acc: 0.9976 - val_loss: 1.2327 - val_acc: 0.8847\n",
            "Epoch 552/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0684 - acc: 0.9969 - val_loss: 1.2746 - val_acc: 0.8923\n",
            "Epoch 553/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0606 - acc: 0.9979 - val_loss: 1.3205 - val_acc: 0.8903\n",
            "Epoch 554/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0664 - acc: 0.9981 - val_loss: 1.2247 - val_acc: 0.8867\n",
            "Epoch 555/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0595 - acc: 0.9976 - val_loss: 1.0776 - val_acc: 0.8893\n",
            "Epoch 556/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0620 - acc: 0.9978 - val_loss: 1.2214 - val_acc: 0.8923\n",
            "Epoch 557/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0603 - acc: 0.9977 - val_loss: 1.1615 - val_acc: 0.8917\n",
            "Epoch 558/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0597 - acc: 0.9975 - val_loss: 1.1382 - val_acc: 0.8873\n",
            "Epoch 559/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0673 - acc: 0.9973 - val_loss: 1.1111 - val_acc: 0.8907\n",
            "Epoch 560/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0607 - acc: 0.9979 - val_loss: 1.0485 - val_acc: 0.8943\n",
            "Epoch 561/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0625 - acc: 0.9975 - val_loss: 1.0362 - val_acc: 0.8917\n",
            "Epoch 562/1000\n",
            "187/187 [==============================] - 9s 50ms/step - loss: 0.0609 - acc: 0.9977 - val_loss: 1.0724 - val_acc: 0.8913\n",
            "Epoch 563/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0611 - acc: 0.9980 - val_loss: 1.1723 - val_acc: 0.8920\n",
            "Epoch 564/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0628 - acc: 0.9977 - val_loss: 1.0678 - val_acc: 0.8890\n",
            "Epoch 565/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0657 - acc: 0.9977 - val_loss: 1.1773 - val_acc: 0.8880\n",
            "Epoch 566/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0626 - acc: 0.9977 - val_loss: 1.1873 - val_acc: 0.8887\n",
            "Epoch 567/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0616 - acc: 0.9983 - val_loss: 1.0942 - val_acc: 0.8930\n",
            "Epoch 568/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0599 - acc: 0.9982 - val_loss: 1.2677 - val_acc: 0.8887\n",
            "Epoch 569/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0668 - acc: 0.9977 - val_loss: 1.2179 - val_acc: 0.8907\n",
            "Epoch 570/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0597 - acc: 0.9974 - val_loss: 1.1362 - val_acc: 0.8957\n",
            "Epoch 571/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0630 - acc: 0.9976 - val_loss: 1.1530 - val_acc: 0.8940\n",
            "Epoch 572/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0636 - acc: 0.9977 - val_loss: 1.0979 - val_acc: 0.8940\n",
            "Epoch 573/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0608 - acc: 0.9975 - val_loss: 1.1861 - val_acc: 0.8870\n",
            "Epoch 574/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0681 - acc: 0.9975 - val_loss: 1.1373 - val_acc: 0.8943\n",
            "Epoch 575/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0642 - acc: 0.9973 - val_loss: 0.8984 - val_acc: 0.8923\n",
            "Epoch 576/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0560 - acc: 0.9980 - val_loss: 1.2031 - val_acc: 0.8927\n",
            "Epoch 577/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0590 - acc: 0.9982 - val_loss: 1.1034 - val_acc: 0.8883\n",
            "Epoch 578/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0594 - acc: 0.9981 - val_loss: 1.1643 - val_acc: 0.8883\n",
            "Epoch 579/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0611 - acc: 0.9977 - val_loss: 1.0745 - val_acc: 0.8890\n",
            "Epoch 580/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0598 - acc: 0.9973 - val_loss: 1.0831 - val_acc: 0.8950\n",
            "Epoch 581/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0594 - acc: 0.9982 - val_loss: 1.1255 - val_acc: 0.8923\n",
            "Epoch 582/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0641 - acc: 0.9975 - val_loss: 0.9460 - val_acc: 0.8943\n",
            "Epoch 583/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0583 - acc: 0.9979 - val_loss: 1.0716 - val_acc: 0.8923\n",
            "Epoch 584/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0611 - acc: 0.9977 - val_loss: 1.2035 - val_acc: 0.8933\n",
            "Epoch 585/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0585 - acc: 0.9979 - val_loss: 1.0606 - val_acc: 0.8943\n",
            "Epoch 586/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0639 - acc: 0.9974 - val_loss: 1.0729 - val_acc: 0.8937\n",
            "Epoch 587/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0594 - acc: 0.9979 - val_loss: 1.2166 - val_acc: 0.8983\n",
            "Epoch 588/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0578 - acc: 0.9981 - val_loss: 1.0593 - val_acc: 0.8937\n",
            "Epoch 589/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0626 - acc: 0.9975 - val_loss: 0.9418 - val_acc: 0.8923\n",
            "Epoch 590/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0621 - acc: 0.9973 - val_loss: 1.0559 - val_acc: 0.8953\n",
            "Epoch 591/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0601 - acc: 0.9979 - val_loss: 1.0434 - val_acc: 0.8963\n",
            "Epoch 592/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0604 - acc: 0.9981 - val_loss: 1.0063 - val_acc: 0.8990\n",
            "Epoch 593/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0619 - acc: 0.9981 - val_loss: 1.1015 - val_acc: 0.8950\n",
            "Epoch 594/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0636 - acc: 0.9977 - val_loss: 1.1672 - val_acc: 0.8933\n",
            "Epoch 595/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0593 - acc: 0.9984 - val_loss: 1.1878 - val_acc: 0.8990\n",
            "Epoch 596/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0643 - acc: 0.9975 - val_loss: 0.9292 - val_acc: 0.8950\n",
            "Epoch 597/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0575 - acc: 0.9982 - val_loss: 1.0252 - val_acc: 0.8913\n",
            "Epoch 598/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0576 - acc: 0.9981 - val_loss: 1.1738 - val_acc: 0.8870\n",
            "Epoch 599/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0581 - acc: 0.9986 - val_loss: 1.2795 - val_acc: 0.8897\n",
            "Epoch 600/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0594 - acc: 0.9980 - val_loss: 1.1739 - val_acc: 0.8920\n",
            "Epoch 601/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0579 - acc: 0.9977 - val_loss: 1.1541 - val_acc: 0.8877\n",
            "Epoch 602/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0603 - acc: 0.9979 - val_loss: 1.0244 - val_acc: 0.8893\n",
            "Epoch 603/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0572 - acc: 0.9984 - val_loss: 1.0777 - val_acc: 0.8913\n",
            "Epoch 604/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0551 - acc: 0.9987 - val_loss: 1.1735 - val_acc: 0.8910\n",
            "Epoch 605/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0550 - acc: 0.9989 - val_loss: 1.1705 - val_acc: 0.8890\n",
            "Epoch 606/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0534 - acc: 0.9986 - val_loss: 1.1389 - val_acc: 0.8900\n",
            "Epoch 607/1000\n",
            "187/187 [==============================] - 9s 49ms/step - loss: 0.0558 - acc: 0.9991 - val_loss: 1.2359 - val_acc: 0.8927\n",
            "Epoch 608/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0541 - acc: 0.9987 - val_loss: 1.1960 - val_acc: 0.8920\n",
            "Epoch 609/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0632 - acc: 0.9977 - val_loss: 1.1491 - val_acc: 0.8893\n",
            "Epoch 610/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0611 - acc: 0.9979 - val_loss: 1.0247 - val_acc: 0.8927\n",
            "Epoch 611/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0602 - acc: 0.9980 - val_loss: 1.0088 - val_acc: 0.8900\n",
            "Epoch 612/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0580 - acc: 0.9979 - val_loss: 1.0304 - val_acc: 0.8917\n",
            "Epoch 613/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0515 - acc: 0.9993 - val_loss: 1.2263 - val_acc: 0.8947\n",
            "Epoch 614/1000\n",
            "187/187 [==============================] - 9s 48ms/step - loss: 0.0568 - acc: 0.9990 - val_loss: 1.1798 - val_acc: 0.8953\n",
            "Epoch 615/1000\n",
            "147/187 [======================>.......] - ETA: 1s - loss: 0.0517 - acc: 0.9995Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAnK7D_Mm_dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "copied_low_accuracy_model.load_weights(file_save_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8ZXIw1Um_ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copied_low_accuracy_model.save('final1_'+str(K)+'_'+file_save_name)\n",
        "# copied_low_accuracy_model.save('final1_Transfer1')\n",
        "copied_low_accuracy_model.save('final1_Random1')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZsEELS6m_WM",
        "colab_type": "code",
        "outputId": "961fcb71-7ddc-44e4-e19f-b5d81ae1a573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "accuracy = copied_low_accuracy_model.history.history['acc']\n",
        "val_accuracy = copied_low_accuracy_model.history.history['val_acc']\n",
        "loss = copied_low_accuracy_model.history.history['loss']\n",
        "val_loss = copied_low_accuracy_model.history.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGIbhYQrBMAzDAEwhGIZhGB7/DySNboyGWUDQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD6ukjIoXbd45AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S9iJ-HGnRFW",
        "colab_type": "code",
        "outputId": "1fa2db8f-c5a8-4a9c-f73b-150decafe624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "copied_low_accuracy_model.evaluate(valid_X, valid_ground, verbose=0)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9976364184220632, 0.8999999998410543]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8o16S6NPmfn",
        "colab_type": "code",
        "outputId": "47b517c4-aecc-4f7a-d9b6-c30f8c47d9f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#testing\n",
        "test_eval = copied_low_accuracy_model.evaluate(test_three_data, test_three_label_one_hot, verbose=0)\n",
        "print('Test loss:', test_eval[0])\n",
        "print('Test accuracy:', test_eval[1])\n",
        "\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.9280055140256882\n",
            "Test accuracy: 0.9106666668256124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z06QupNsYeLZ",
        "colab_type": "code",
        "outputId": "b52e1b8c-93ab-420d-c540-6b84a15d9abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "layer_names"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['conv8_1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRuSLttWnRCR",
        "colab_type": "code",
        "outputId": "965406fc-f99d-46c5-ee6d-8338b2b89f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "# now get DI score of a layer from high accuracy and low accuracy model\n",
        "#layer_name = layer_to_replace\n",
        "discores_low_acc_modified_model = createCNNLayerDIScorePlot(modelFilt, layer_to_replace)\n",
        "\n",
        "plt.ylim([0,1.0])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-4cb67efdc4fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiscores_low_acc_modified_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateCNNLayerDIScorePlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelFilt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_to_replace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'modelFilt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI4AzYZDnQ_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now get DI score of a layer from high accuracy and low accuracy model\n",
        "#layer_name = 'lowAccuracyModelConv2_1'\n",
        "discores_low_acc_modified_model = createCNNLayerDIScorePlot(modelFilt, layer_to_replace)\n",
        "\n",
        "plt.ylim([0,1.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9AxL2k2cPvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now get DI score of a layer from high accuracy and low accuracy model\n",
        "layer_name = 'conv2d_1'\n",
        "discores_low_acc_modified_model = createCNNLayerDIScorePlot(modelFilt, layer_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmomNWqXclu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now get DI score of a layer from high accuracy and low accuracy model\n",
        "layer_name = 'conv2d_2'\n",
        "discores_low_acc_modified_model = createCNNLayerDIScorePlot(modelFilt, layer_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iC11tO-c1ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now get DI score of a layer from high accuracy and low accuracy model\n",
        "layer_name = 'conv2d_3'\n",
        "discores_low_acc_modified_model = createCNNLayerDIScorePlot(modelFilt, layer_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3SsO1CXc09Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now get DI score of a layer from high accuracy and low accuracy model\n",
        "layer_name = 'conv2d_4'\n",
        "discores_low_acc_modified_model = createCNNLayerDIScorePlot(modelFilt, layer_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unii3onHcpx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2XDOWtOP2p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelFilt.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxnMxP6cTGLO",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CRji91-FYcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}